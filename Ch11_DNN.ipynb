{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch11_DNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"IN-2dxGUcG1b","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","%matplotlib inline\n","# to make this notebook's output stable across runs\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PG55tqgLhwyv","colab_type":"text"},"cell_type":"markdown","source":["#Vanishing/Exploding Gradients Problems\n","The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once\n","the algorithm has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n","\n","Vanishing gradients problem: the gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.\n","\n","Exploding gradients problem: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges."]},{"metadata":{"id":"8PGQ0kvujCgg","colab_type":"code","colab":{}},"cell_type":"code","source":["def logit(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0XfhknYfjEWA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"9ed331b8-5e83-40c1-d171-004507321b21","executionInfo":{"status":"ok","timestamp":1540858825889,"user_tz":240,"elapsed":726,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["z = np.linspace(-5, 5, 200)\n","\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [1, 1], 'k--')\n","plt.plot([0, 0], [-0.2, 1.2], 'k-')\n","plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n","plt.plot(z, logit(z), \"b-\", linewidth=2)\n","props = dict(facecolor='black', shrink=0.1)\n","plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.grid(True)\n","plt.title(\"Sigmoid activation function\", fontsize=14)\n","plt.axis([-5, 5, -0.2, 1.2])"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-5, 5, -0.2, 1.2]"]},"metadata":{"tags":[]},"execution_count":92},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXgAAAEJCAYAAACe4zzCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f747879fe10>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"XZ-xll0bjHN9","colab_type":"text"},"cell_type":"markdown","source":["The logistic activation (sigmoid) function saturates at 0 or 1 when the inputs become large (negative or positive), with a derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."]},{"metadata":{"id":"B0XndpHzjeoO","colab_type":"text"},"cell_type":"markdown","source":["##Xavier and He Initialization\n","To propagate the signal properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the variance of the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n","\n","Note: the book uses **tensorflow.contrib.layers.fully_connected()** rather than **tf.layers.dense()** (which did not exist when this chapter was written). It is now preferable to use **tf.layers.dense()**, because anything in the contrib module may change or be deleted without notice. The **dense()** function is almost identical to the **fully_connected()** function. The main differences relevant to this chapter are:\n","\n","* Several parameters are renamed: **scope** becomes **name**, **activation_fn** becomes **activation** (and similarly the **_fn** suffix is removed from other parameters such as **normalizer_fn)**, **weights_initializer** becomes **kernel_initializer**, etc.\n","* the default activation is now **None** rather than **tf.nn.relu**.\n","* it does not support **tensorflow.contrib.framework.arg_scope()** (introduced later in chapter 11).\n","* it does not support regularizer params (introduced later in chapter 11).\n","\n","####Xavier initialization (when using the logistic activation function.. sigmoid)\n","* Normal distribution with mean 0 and standard deviation $\\sigma=\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$,\n","* Uniform distribution between -r and +r, with $r=\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$\n","\n","####He initialization (when using the ReLU and its variant activation functions)\n","* Normal distribution: $\\sigma=\\sqrt{2}\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$\n","* Uniform distribution: $r=\\sqrt{2}\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$\n","\n","####Initialziation strategy for hyperbolic tangent,\n","* Normal distribution: $\\sigma=4\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$\n","* Uniform distribution: $r=4\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$"]},{"metadata":{"id":"Rex2Difjnfca","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28* 28 #MNIST\n","n_hidden1 =300\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IWf3Yi2Dnrl0","colab_type":"code","colab":{}},"cell_type":"code","source":["he_init = tf.variance_scaling_initializer()\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name='hidden1')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oo9c0oa28_Oh","colab_type":"text"},"cell_type":"markdown","source":["##Nonsaturating Activation Functions\n","The main problem for ReLU is that the gradient of the ReLU function is 0 when its input is negative and then the neurons are dead and they start to output 0.\n","\n","To solve this problem, use variants of ReLU functions."]},{"metadata":{"id":"paf4ZzS1_QgW","colab_type":"text"},"cell_type":"markdown","source":["###Leaky ReLU\n","$LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$,\n","\n","where $\\alpha$ is a hyperparameter defining how much the function \"leaks\", which is the slope of the function for z < 0. Typically set to 0.01\n","\n","###Randomized leaky ReLU (RReLU)\n","$\\alpha$ is picked randomly in a given range during training, and it is fixed to an average value during testing.\n","\n","###Parametric leaky ReLU (PReLU)\n","$\\alpha$ is authorized to be learned."]},{"metadata":{"id":"eYBok26TAYk2","colab_type":"code","colab":{}},"cell_type":"code","source":["def leaky_relu(z, alpha=0.01):\n","  return np.maximum(alpha*z, z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yWnH3lghAdf_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"05def5a4-1ebc-4991-9336-daf778f739ff","executionInfo":{"status":"ok","timestamp":1540858828493,"user_tz":240,"elapsed":717,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([0, 0], [-0.5, 4.2], 'k-')\n","plt.grid(True)\n","props = dict(facecolor='black', shrink=0.1)\n","plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.title(\"Leaky ReLU activation function\", fontsize=14)\n","plt.axis([-5, 5, -0.5, 4.2])"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-5, 5, -0.5, 4.2]"]},"metadata":{"tags":[]},"execution_count":96},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWkAAAEJCAYAAABWj9YUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcU9X9//HXTTKZzMCwOvKlFMQF\njnX5VtH6dRetiFrcKGptcfm5sYqAIFBRUBDZR/bFlq9SF6RV+bpV674UtYj6UEFPFaWIyib7QGZL\nfn/cAAPMEoaZuTfJ+/l4zIPJTebmk0vyzsnJOfc48XgcERHxp4DXBYiISOUU0iIiPqaQFhHxMYW0\niIiPKaRFRHxMIS0i4mMhrwuQ2mGMeRhoaK3t5nUtqcAYczbwD6C5tbbQ7/stt/8FwCXAHdba2bW9\n/0rus04fk1TN0Tjp+mOMWQlMtNZOr4N9P0wthLQxZiRwD1BcbvMO4CNgmLV2SW3UU9mxMMZ0BN4A\n8qy12w+s+mpruhxYbq39dyrst4L7+SXwCdDBWvtxHd9XvTwmqZ66O6QiH1lrI7t+gLa4If2SMeZQ\nb0s7KKOA9im03301Sfz7VT3cV309JqmGujt8xBhzDvAAcBxQCPwZuMdaGzPGOLgvnGuB5sBKYKi1\n9vlK9jUC+D1wK/Aq0Mpauy5xXQhYC/S01v61urqstVuNMUOAnkBHYGFiP72A23BD/LtErU/W5LEn\nyxjTAZgM/BIoBf4O9LHWbit3/TTgBOBHYJS19hFjzDLgGODpRJfBPBItduA14B/W2rvL3c/9wHnW\n2tOqus+q9mut3W6MaZWo5ywgO3FdH2vtamNMW+Bb4AJgPG4ofgz8zlq7ep/H3Ql4IXFxgzFmENAN\n+NBaOyhxm137O95a+3ni08r9wOXAOYnjcau19o2aHqvafEySHLWkfcIY83PgeeBPQDPg10B34JbE\nTboDPXBfbI2AWcACY0yTCvZ1NdAbuMha+xbwH+B35W7SEQgCzx1AicHEz677uBwYDdyAG3R3APON\nMb84gH3WxELgfeAQ3Dezk4GhiZpycY/hs7jH8CZgjjHmFGvtsYm/72qtva6CfV6+z7auwILq7rOa\n/QI8g9t1dBTum1kW8Pg+t+kPXAwchvsGPHDfnVhrX8ENPoBDDqDLbBAwEvd4/AuYBAd1rGrtMUly\nFNL+cQ1grbXzrLWl1trlwFTcEAT3RXCUtXaltTYGPAE0APYKRWPMKcBM4FJr7TeJzfNxQ36X3wJP\nWWujyRRmjGmO25IsBF5KbL4F+F9r7b+stWWJFv3LQEUv6tp0Am6Lvcxauxb3U8LJies6A7nAJGtt\nUeINqhvwUzX7/CtwrDHmSABjzDG4LcCFSdxnpRJ9yL8CBllrt1hrN+IG5lnGmJblbjrXWvujtXYD\nbqu0Nt/oXrTWLrHWFgP/V27fNTpWPnlMGUXdHf5xJHCiMaZ8cDq43RLgBvJkY8zFQNNyt8ku9/vP\ncF+ID1trPyi3fT4w0hjTHvgauIK9W9b76rBPHdm4ra6O1tqt5eq9wBjTt9ztAsCWKvZbG84Hhhtj\nDG4LLgS8W66m1dba0l03rqw7qDxr7SpjzAe4relJuK3ot6y1PyZxn1U5Ati2z8f8rxP/tsXtYgC3\ne2CXHUBOEvtO1r77jiR+r9Gxwh+PKaOoJe0fO3H7RSPlfrKttW0S188ATsXtqsgBWlSwj//Bbc3e\nuqtVCGCtXQm8jdtHfQbuR9W3qqhl9xeHuF0rNrFt+T71Dt+n3rC19tokH28xbktuX42BOLBfK98Y\nczRuq/cJoEWivmnlbhKj5s/p8l0eXRP3kcx9ViW7iuvKD6uKHViplQpWsK2yfdf0WNX3Y8p4Cmn/\n+Bo4zhiz+//EGHOoMWZXC+QU4FHrigMnVbCPZ621N+D2Gc43xpR/0T6CGz5XAY8n9lGtxMfkHsBQ\nY8x/71Nv+csYY9qUr78aXwIdKth+GrCsfAuvnBNxv7ibbK3dkdhW/jh8AxxmjNnVWsQYc40x5n+S\nqOevwKnGmJNx+52fSvI+q7ICyEt80bbL0bhhtiLJfVQlyt5vdEdWdsMK1PRY1fVjkn0opP3jcdxW\n60hjTK4xpg3wIjAscf03wMnGmHDiW/k+QBFQ/sVSlvi3H9AGuLPcdX/DfRFfCzx6IIUl+isfB+aV\nC/5ZQDdjzGXGmJAx5nTcMbznJrnbCcAVxpjexpiGxpgGxpjuidoHVPI33+C25E4yxjQyxtyD2w30\nX4m6/g5sBe4xxuQkanqIPc/zKNDOGNOogse4GveLtcm4n2g2JnmfVe33Q+BzYELiMR4K3IfbT7w+\nyeNUla+A84wxzY0x+bhfFierpseqrh+T7EMhXf8KjDHRfX46WWs3AZcCF+F+efMebhfFqMTfDQHa\nAZtxv1AcitvX/FCin3o3a+1m4EZghDHmhMS2bcDTwLfW2s9rUPdg3D7HwYn9vY77DX4BsA13mNZg\na+1r5f7migoe66jE37+D23VzKW7/5XdAL+Aaa+2rFRWQ6GcvwP3i7t9ACfD/cPvo37LWFgHnJX42\n4n566GutfS+xi1m4QxyfqOQxLsQdVrZrVEe191nVfhOfVi5P3HYl7lC0lcAfKrn/AzUB97nyHe6X\nc1OS/cOaHqt6eEyyD804zCDGmNeAp621M7yuRUSSo9EdGSAxEeZmwOC2mEQkRSikM8MO3L7V39pa\nPh+GiNQtdXeIiPiYvjgUEfGxWu/uWL9+m+dN86ZNc9m0aUf1N8wAOhauk046jkDAYcmSz7wuxRf0\nvNijomMxYUKYCROyadEixptv7qB587qPtfz8PKei7WnZkg6FKpp4lZl0LKQiel7sse+xeP/9IJMm\nhXGcODNnRusloKuSliEtIlITmzdDr14RYjGH224r5qyzyqr/ozqmkBYRAeJxGDgwwvffB+jQoYwh\nQ4qr/6N6oJAWEQEefTSL55/PomHDOLNn7yQry+uKXAppEcl4//53gOHD3RP8jR8fpW1bz8c/7JZU\nSCdOwLLCGHNDHdcjIlKvolHo0SPCzp0OV15ZQrduFZ2A0TvJtqSH456ERUQkrQwdCsuWBTn88Bjj\nxiW1WFG9qjakEyc9P4Y9i2CKiKSFV14JMmUKhEJuP3TDhl5XtL9kWtKT0CKSIpJm1q516NfPXfNg\n2LBiTjzRn4vJVDnj0BhzHfCetfZbd3m36jVtmuuLgfL5+Xlel+AbOhYQCLiTuXQs9sjkYxGLwe9/\nDz/9BOefDyNHZhMIVLUymHeqmxb+G+AIY0wX4OdAkTFmdWUnZQd8MdU0Pz+P9eu3eV2GL+hYuGKx\nOIGAo2ORkOnPi+nTs3j11QjNm8eYPz/ATz95fywqe9OsMqSttVfv+t0YMxJYWVVAi4j43ccfBxgz\nxm01T50apWXLXNb7eOEvjZMWkYyxfTv06JFDaanDLbcU06mT99O+q5P0WfCstSPrsA4RkTo3dGiE\nlSsDHHtsGXffXeR1OUlRS1pEMsLf/hZi4cIscnLizJkTJRLxuqLkKKRFJO2tXOlw551uKo8eXUT7\n9v4cblcRhbSIpLWSEujZM4ft2x26dCmhe/cSr0s6IAppEUlr48eH+eijIK1axZg8OYpT4fon/qWQ\nFpG09c47QaZODRMIxJk1K0qTJl5XdOAU0iKSln76yaF37wjxuMPAgcWceqr/h9tVRCEtImknHof+\n/SOsXRvglFNKGTjQH6us1IRCWkTSzrx5Wbz8cojGjd1ujlDSM0L8RyEtImll+fIAI0e6074nT47S\nurV/VlmpCYW0iKSNHTvcVVaKihy6dy/mkkv8tcpKTSikRSRtjBiRjbVB2rUrY9So1Jj2XR2FtIik\nhRdeCPHII2HC4TizZ0dp0MDrimqHQlpEUt733zsMGOBO+77nniKOPz51pn1XRyEtIimtrAz69Imw\nebPD+eeXcsstqTXtuzoKaRFJaVOmhFm8OER+fowpU1Jv2nd1FNIikrL+9a8AEyaEAZgxI0p+fmoP\nt6uIQlpEUtKWLdCrVw5lZQ59+hTTsWNqTvuujkJaRFJOPA6DB0f47rsAJ5xQxrBh6THcriIKaRFJ\nOQsWhFi0KIvc3DizZ+8kHPa6orqjkBaRlPL11w7DhrnD7caNi3LEEenXD12eQlpEUkZRkbva944d\nDl27lnDVVak/7bs6CmkRSRn335/NZ58FadMmxoQJ6TfcriIKaRFJCa+/HmT27DDBoNsPnZfndUX1\nQyEtIr63bp1D375uP/TQocWcfHL6TPuujkJaRHwtFoPbbouwYUOAM88spW/f1F1lpSYU0iLia3Pm\nZPHGGyGaNYsxY0aUYNDriuqXQlpEfOvTTwOMHu2uslJQUETLluk93K4iCmkR8aXt293hdiUlDjfe\nWMxFF6X/cLuKKKRFxJfuuivCihUBfvGLMkaMSN9p39VRSIuI7yxaFOKJJ7KIROLMmRMlJ8friryj\nkBYRX1m1yuGOO9zhdvfdV8TRR2fOcLuKKKRFxDdKS6Fnzxy2bXO46KISrr8+vVZZqQmFtIj4xsSJ\nYT78MEjLljEKCjJj2nd1FNIi4guLFwcpKAjjOHFmzozSrJnXFfmDQlpEPLdpE/TuHSEedxgwoJgz\nzkjPVVZqQiEtIp6Kx2HAgAg//BDg5JPLGDQos6Z9V0chLSKeeuSRLF58MYu8PPfsdqGQ1xX5i0Ja\nRDzz5ZcB7rnHnfY9cWKUNm0yb9p3dap9zzLG5AIPAy2ACDDKWvt8HdclImlu507o0SNCNOpwzTUl\nXHFFZk77rk4yLelLgA+ttecAVwGT67YkEckE996bzRdfBDnyyBj33x/1uhzfqrYlba19stzF1sDq\nuitHRDLBSy8FmTcvTFZWnDlzdtKwodcV+VfSXfTGmMXAz4EuVd2uadNcQiHvT/ian58ha+skQccC\nAgF3VoSOxR5eHYvvv4cBA9zfH3jA4de/buBJHeX5+XnhxOPJd9QbY04A5gO/tNZW+Ifr12/zvOc/\nPz+P9eu3eV2GL+hYuE466TgCAYclSz7zuhRf8Op5UVYGV16Zw7vvhjj33FKeeGInAY+HL/jlNZKf\nn1fh/MpqD48x5iRjTGsAa+0nuK3v/NotT0QywfTpYd59N8Qhh8SYNi3qeUCngmQO0dnAHQDGmBZA\nQ2BDXRYlIuln6dIAY8eGAZg+Pcqhh3r+oTslJBPSs4FDjTHvAC8Afay1mX3uQBE5INu2uauslJU5\n9OhRzHnnadp3spIZ3bET+H091CIiaSgeh8GDI6xaFeD448sYPjxzV1mpCfUIiUidWrgwxNNPZ5Gb\n6w63y872uqLUopAWkTrzzTcOQ4e6q6yMGRPlqKPUD32gFNIiUieKi91VVgoLHS67rIRrrtG075pQ\nSItInRg7NswnnwRp3TrGxIlaZaWmFNIiUuvefDPI9OnZBINxZs3aSePGXleUuhTSIlKrNmxw6NvX\n7YceNKiYU07RiN2DoZAWkVoTj8Ptt0dYty7AaaeV0r+/Vlk5WAppEak1f/pTFq+8EqJJE3cx2aD3\n51pLeQppEakVn30W4N573UHQBQVRWrXScLvaoJAWkYNWWAg9e0YoLna47rpifvMbDberLQppETlo\n99yTzVdfBTGmjPvu07Tv2qSQFpGD8txzIf7ylzDZ2XHmzImSm+t1RelFIS0iNbZ6tcPAge5wu5Ej\nizjmGA23q20KaRGpkdJS6NUrwpYtDp07l3LjjSVel5SWFNIiUiMFBWE++CBEixYxHnxQ077rikJa\nRA7Y++8HmTQpjOO446GbN9dwu7qikBaRA7J5s9vNEYs53HZbMWedpVVW6pJCWkSSFo/DHXdE+P77\nAB06lDFkiKZ91zWFtIgk7bHHsnjuuSwaNnTPbpeV5XVF6U8hLSJJ+fe/A9x1lzvte/z4KIcfrn7o\n+qCQFpFqRaPQo0eEnTsdrryyhG7dNO27viikRaRao0dns2xZkLZtY4wbF/W6nIyikBaRKr3ySpC5\nc8OEQu5q3w0bel1RZlFIi0il1q516NfPnfY9bFgxJ56oad/1TSEtIhWKxaBPnwg//RTg7LNL6dNH\nw+28oJAWkQrNnJnF22+HaN48xowZUQJKC0/osIvIfj7+OMCYMe5wu6lTo7RooeF2XlFIi8hetm+H\nHj1yKC11uOWWYjp10rRvLymkRWQvQ4dGWLkywLHHlnH33VplxWsKaRHZ7amnQixcmEVOjrvKSiTi\ndUWikBYRAFaudBg82E3l0aOLaN9ew+38QCEtIpSUQM+eOWzf7tClSwndu2uVFb9QSIsI48eH+eij\nIK1axZg8Waus+IlCWiTDvfNOkKlTwwQCcWbNitKkidcVSXkKaZEM9tNPDn36RIjHHQYOLObUUzXc\nzm8U0iIZKh6H/v0jrFkT4JRTShk4UNO+/UghLZKh5s3L4uWXQzRq5HZzhEJeVyQVSeq/xRgzHjgr\ncfsHrLVP12lVIlKnPvsMRo50p31PnhyldWtN+/aralvSxphzgeOstacBFwIP1nlVIlJnduyAa66B\noiKH7t2LufRSrbLiZ8l0d7wNXJn4fTPQwBgTrLuSRKQujRiRzbJl0K5dGaNGadq33znxePIfc4wx\ntwJnWWuvrew2paVl8VBIGS7+0rZtWwBWrlzpaR1ee+YZ6NoVwmH44AM44QSvK5JyKhydnvRXBcaY\ny4CbgAuqut2mTTsOrKw6kJ+fx/r127wuwxd0LFyxWJxAwMnoY/HDDw433dQAcBg/Hlq12sb69V5X\n5T2/vEby8/Mq3J7U6A5jTGfgLuAia+2WWqxLROpBWRn07h1h0yaH888vpV8/ryuSZCXzxWFjYALQ\nxVq7se5LEpHaNmVKmMWLQ+Tnx5gyRdO+U0ky3R1XA4cAC40xu7ZdZ61dVWdViUitWbIkwIQJYQCm\nT4+Sn6/hdqmk2pC21s4F5tZDLSJSy7Zscc9uV1bm0KdPMeeeq2nfqUYzDkXSVDwOgwdH+O67ACec\nUMawYRpul4oU0iJpasGCEIsWZZGbG2f27J2Ew15XJDWhkBZJQytWOAwb5q6yMnZslCOOUD90qlJI\ni6SZoiK49dYcduxw6Nq1hKuv1rTvVKaQFkkz99+fzWefBWnTJsaECRpul+oU0iJp5PXXg8yeHSYY\ndPuh8yqexCYpRCEtkibWrXPo29fthx46tJiTT9Zq3+lAIS2SBmIx6NcvwoYNAc48s5S+fbXKSrpQ\nSIukgTlzsnj99RDNmsWYMSNKUCeiTBsKaZEU9+mnAUaPdldZKSgoomVLDbdLJwppkRS2fTv06JFD\nSYnDjTcWc9FFGm6XbhTSIils+PBsVqwI8ItflDFihKZ9pyOFtEiKWrQoxOOPh4lE4syZEyUnx+uK\npC4opEVS0KpVDnfc4Q63u/feIo4+WsPt0pVCWiTFlJa6px/dts3hootKuOGGEq9LkjqkkBZJMRMn\nhvnwwyAtW8YoKNC073SnkBZJIYsXBykoCOM4cWbOjNKsmdcVSV1TSIukiE2b3MVk43GH/v2LOeMM\nrbKSCRTSIikgHocBAyL88EOAk08uY9AgTfvOFAppkRQwf34WL76YRV6ee3a7rCyvK5L6opAW8bkv\nvwxw993utO+JE6O0aaNp35lEIS3iY9Eo9OgRIRp1+N3vSrjiCk37zjQKaREfu/febL74IsgRR8QY\nMybqdTniAYW0iE+9/HKQP/85TFZWnLlzd9KwodcViRcU0iI+9OOPDrff7k77vuuuIv77vzXtO1Mp\npEV8pqwM+vaNsHFjgI4dS+nZU9O+M5lCWsRnZswI8847IQ45JMa0aVECepVmNP33i/jI0qUBHngg\nDMD06VFatNBwu0ynkBbxiW3b3FVWysocevQo5rzzNO1bFNIivnHnnRFWrQpw/PFlDB+uVVbEpZAW\n8YGFC0M89VQWublx5szZSXa21xWJXyikRTz2zTcOQ4a4w+3GjIly1FHqh5Y9FNIiHioudldZKSx0\nuOyyEq65RtO+ZW8KaREPjR0b5pNPgrRuHWPiRK2yIvtTSIt45M03g0yfnk0wGGfWrJ00bux1ReJH\nCmkRD2zY4NC3r9sPPWhQMaecomnfUjGFtEg9i8fh9tsjrFsX4LTTSunfX6usSOWSCmljzHHGmBXG\nmL51XZBIuvvTn7J45ZUQTZq4i8kGg15XJH5WbUgbYxoA04DX6r4ckfT2+ecB7r3XHQRdUBClVSsN\nt5OqJdOSLgIuBn6o41pE0lphobvKSnGxw3XXFfOb32i4nVQvVN0NrLWlQKkxJqkdNm2aSyjk/ee3\n/Pw8r0vwDR0LCATcsW1eHou77oKvvoJjjoFZs8Lk5oY9qwX0vCjPz8ei2pA+UJs27ajtXR6w/Pw8\n1q/f5nUZvqBj4YrF4gQCjmfH4rnnQjz0UA7Z2XFmzNhBYWGMwkJPSgH0vCjPL8eisjcKje4QqWOr\nVzsMHOgOtxs5sohjj9VwO0meQlqkDpWWQq9eEbZscejcuZQbb9QqK3Jgqu3uMMacBEwC2gIlxphu\nQFdr7cY6rk0k5RUUhPnggxAtWsR48EFN+5YDl8wXh0uBjnVfikh6ef/9IJMmhXGcODNmRGneXMPt\n5MCpu0OkDmzeDL17R4jFHG67rZizz9YqK1IzCmmRWhaPwx13RFi9OkCHDmUMGaJp31JzCmmRWvbY\nY1k891wWDRu6Z7fLyvK6IkllCmmRWvTVVwGGD3enfY8fH+Xww9UPLQdHIS1SS6JRuPXWCDt2OFx5\nZQndumnatxw8hbRILRk9Optly4K0bRtj3Lio1+VImlBIi9SCV18NMndumFDIXe27YUOvK5J0oZCu\nofvvH8nw4Xd6XYb4wNq1Dv36udO+hw0r5sQTNe1bak/ah3S3bpfw1FNPel2GpKlYDPr0ibBhQ4Cz\nzy6lTx8Nt5PalfYhLVKXZs7M4u23QzRvHmPGjCgBvaKklmX0U+rjj5fSs+eNXHDBOVx2WWfmzp1J\nLOZ+VI3H48ydO5Pf/rYLnTqdxbXXXsU///lOpfuaN28u11zTlS1bNtdX+eKxTz4JMGaMO9xu6tQo\nLVpouJ3UvowN6XXr1nLnnQPo0uUyXnzxNR58cBb/+MffefbZZwB4+eUXefbZp5k+fS4vv/wWl1/e\njZEj/8i2bfufd/a11/7BM8/8jYkTp9K4cZP6fijige3boUePHEpLHW65pZhOnTTtW+pGxob0q6++\nTJs2h9Gly2WEQiEOP/wIunW7mr///XkAOnW6kAULFtGy5c8IBAJ06tSZnTt38p//fLvXfpYv/5xJ\nk8YxduxkWrX6uRcPRTwwdGiEb78NcOyxZdx9d5HX5Ugaq/WVWVLF99+v5quvLOedd/rubfF4nKZN\nmwEQje5k2rTJvP/+P/dqPRcX7/liaMOGDQwbdgcXXdSFY489rv6KF0899VSIhQuzyMmJM2dOlEjE\n64oknWVsSGdnZ/OrX53KpElTK7x+8uRxWGuZNm0OrVsfRmFhIRde2HGv2yxf/jmdO1/Ms88+Q9eu\nV6olnQFWrnQYPNhN5dGji2jfXsPtpG5lbHdHq1at+fbbFbu/KATYtGkjRUXuTLHly5fRufNFtGnT\nFsdxsPaL/fZxxhlnc9ddIzn77I6MHn0PZWXql0xnJSXQq1cO27c7dOlSQvfuWmVF6l7GhnSnThdS\nWLidefPmEo1GWbNmDYMG3c5f/vIwAD/72c/58svllJSUYO2XPP30QsLhMBs2rN+9j2DQPXz9+w9m\n7dq1PP74fC8eitST8ePDLF0apFWrGJMna5UVqR8Z0d0xdepkZsyYste2ceMmM3asu/3xx/9Co0aN\n+PWvO3HDDTcD0KvXbYwadTcXXtiR9u2P5o9/HEGjRk0YN240eXl7r+qbl5fHsGF3M2TIQE499XTa\ntTP19tikfrzzTpCpU8MEAnFmzYrSRIN4pJ448Xjtju1cv36b54NF/bJEux/oWLhOOuk4AgGHJUs+\nO+C//eknh3PPzWXNmgCDBhVx552pP6tQz4s9/HIs8vPzKvxslrHdHSLJiMdhwIBs1qwJcMoppQwc\nmPoBLalFIS1ShXnzsnjppSwaNXK7OUIZ0UEofqKQFqnE8uUBRo50p31PnhyldWvPe/IkAymkRSqw\ncyf07BmhqMihe/diLr1Uq6yIN1I6pN97bzGffPKR12VIGhoxIpsvvwzSrl0Zo0Zp2rd4J2VDesGC\nx7j11hu4/fY+rFnzo9flSBp54YUQDz8cJhyOM3t2lAYNvK5IMlnKhXQ8HueBB+5j6NBBrF27hi++\nWEbv3rcQjWpNOTl4P/zgMHCgO+377ruLOP54TfsWb6VUSEejUXr3vpkHH5zEjh2Fu7e/++7b3HPP\nMA8rk3RQVga9e0fYtMnh/PNLufVWTfsW76XMgKI1a36kV69b+Oc/397vutNPP4sBAwZ7UJWkk6lT\nwyxeHCI/P8aUKZr2Lf6QEi3ppUuXcPXVXfcLaMdx6Nq1GwsWPEXLlj/zqDpJB0uWBBg/PgzA9OlR\n8vM13E78wfch/cwzf+Pmm6/niy+W7bU9N7cBt98+kFmz/kxEJ/SVg7B1q3t2u7Iyhz59ijn3XJ3N\nUPzDt90d8XicKVMmMX36g2zdunWv6w49tAXDht3DH/5wrUfVSbqIx2HQoAirVgU44YQyhg3TcDvx\nF1+GdElJCYMH9+fJJx/f7xzN7dq1Z/z4BznjjDM9qk7SyYIFIRYtyiI3N87s2TsJh72uSGRvvgvp\njRs30qvXTbzxxmv7XferX/0PM2Y8RNu2beu/MEk7K1Y4DBvmdpWNHRvliCPUDy3+46s+6eXLP+eq\nqy6vMKAvvvgSFi5cpICWWlFU5K72vWOHQ9euJVx9taZ9iz/Ve0i/9dbrFS4z9dJLL3Dddb/n008/\n2Wt7dnY2PXr0Zt68v9BAU7+klowZk82nnwZp0ybGhAkabif+Va8hXVZWxogRdzF8+JC9ts+aNZ1+\n/XqzatXKvbY3b34II0aMZtSosQQCvmr0Swp7/fUgs2aFCQbdfuh9FtoR8ZV67ZNeuPAJli9fxtdf\nf8Vhh7Xlllt68cc/3sljjz1CcfHeJ1M//PAjeOCBCZx3Xqf6LFHS3Lp1Dn37uv3QQ4YUc/LJmvYt\n/lavIb1o0VMAFBcXU1AwgdfhK1rpAAAHHklEQVRee4W33npjv9udcEIHpk2bjTFH12d5kgH69Yuw\nYUOAM88s5bbbtMqK+F9SIW2MKQBOBeLA7dbaJQd6R//61/u8997i3Zc3bdpUYUB36tSZmTMfonFj\nrfQptWvrVnj99RDNmsWYMSNKMOh1RSLVq7aj1xhzDtDOWnsacBMwtSZ3NH/+/xKN7qz0+qysLK6/\n/ibmz1+ggJaDVlrqLiC7YoXD0qUBNm502LTJva6goIiWLTXcTlJDtauFG2PuA1ZZa/+UuPwlcIq1\ndmtFt2/T5rD9dlhaWsratWuIxSrv/2vcuDGNGjU+kNorFQg4xGJ6EULqH4tYrLofp8Lt+z+tVwPQ\ntGkrGjas94fhO6n+vKhNfjkWq1b9p8IxRsl0d/wXsLTc5fWJbRWGdCCw//0UFm6vMqDBDfKK/ram\nanNfqc4Px2JXeJaV7R+oVW07GIHAnp+SEvffRo28PxZ+4YfnhV/4+VjU5IvDKh/NkiWf7XW5pKSE\n888/i61bl1e502g0yh/+cD39+w+qQUl7y8/PY/36bQe9n3RQW8ciHnfX/du82dn9s2mTw5YtsGmT\nU8F2Z/f2rVtr/gLIyYnTpMm+P9CkSZymTeM0buz+W/76pk3j5OW5obzLSScdRyDg7Pf8zFR6jezh\n92ORTEj/gNty3uVnQNLrVT366CN88UXVAQ3uGOpp0wo46qj2dOlyabK7lwNUVuZ+gVZdsJYP313X\nFRXVLGwdJ07jxtUHa+PG7N6+63Y6waFkumRC+h/AvcAcY0wH4AdrbdJvO88//3+VXhcIBGjb9nCM\nOZr27X/B6aefzhlnnJ3srjNaRa3azZvZL3QLC2Hdutzd27duhXi8ZmGbnb1/uCbTqm3UaO9WrYgk\nr9qQttYuNsYsNcYsBmJAn2R3/vbbb/LBB+/tvhwOhznyyKNo3/5ojDmac889nxNP7JCxswl3tWor\na83u+dn/NtHogQTtnrFmbqs2XqNWbU5O7R8DEalaUn3S1tqhNdn5vHlzOeaYY3eHcufOF9OuXXuc\nNDtRQjRKBSFbffhu2VLzVm04vHe47grWXb/v2t62bQ6OU7g7jBs1QuODRVJInc44nDZtNnl5jery\nLmpNLJZcq3bTJhIBu+e6A2vV7q1Ro4pbtXtfZp8wdlu1ybzX5efD+vWa+iySquo0pL0I6GgUfvwR\nvv46sFdXQeWh627fvPngWrV7dxewT79tfL8WbpMmbstXrVoRqYrvTvoPbqt22zaqCdb9Ryhs3uyw\nc+euoD3w05rm5VX0xVj5Vi0Vbs/NTa5VKyJyoOo0pIuK9u0+qDhY973N5s0OsVjNUi8rK06zZg6N\nG5ft9eXX/oG79wiFxo3jhHz5liUimazWY6ljxz3DvXbsqHnzMi+vokkM+4882LcrITcXDj00j/Xr\nd9TioxIR8Uath/Ty5Xs6WUOh6oN1/y/L3FZtVlZtVyYiknpqPaRfe61wd+g2aKC+WhGRg1HrIX38\n8RruJSJSWzJzqp+ISIpQSIuI+JhCWkTExxTSIiI+ppAWEfExhbSIiI8ppEVEfEwhLSLiYwppEREf\nc+LxuNc1iIhIJdSSFhHxMYW0iIiPKaRFRHxMIS0i4mMKaRERH1NIi4j4mEJaRMTH0np9bGNMC+BL\n4Apr7Zsel+MJY0wI+DNwJO7/9yBr7bveVlX/jDEFwKlAHLjdWrvE45I8Y4wZD5yF+3x4wFr7tMcl\necoYkwN8Doyy1j7scTn7SfeW9ATgG6+L8Ni1QKG19kzgJmCyx/XUO2PMOUA7a+1puMdgqsclecYY\ncy5wXOJYXAg86HFJfjAc2Oh1EZVJ25A2xpwHbAM+87oWjz0KDEz8vh5o7mEtXvk1sAjAWvsF0NQY\n08jbkjzzNnBl4vfNQANjTNDDejxljDkaOAZ4wetaKpOW3R3GmDAwAriMDG8pWGtLgJLExf7A4x6W\n45X/ApaWu7w+sW2rN+V4x1pbBhQmLt4EvJjYlqkmAX2B670upDIpH9LGmJuBm/fZ/HfgIWvtZmOM\nB1V5o5JjMcJa+7Ixpg/QAbik/ivzHcfrArxmjLkMN6Qv8LoWrxhjrgPes9Z+6+ecSMsTLBlj/gns\n+gh3JG7L6Upr7TLvqvKOMeYm3I+4l1tro17XU9+MMSOBH621cxKXvwF+aa3d5mlhHjHGdAZGARda\na33bF1vXjDFPAkcAZcDPgSKgh7X2VU8L20dahnR5xpiHgYczeHTHEcCTwDnW2h1e1+MFY8zpwL3W\n2k7GmA7A1MQXqRnHGNMYeAc431q7zut6/CLxRr7Sj6M7Ur67Q6p1M+6XhS+W+0h3gbW22LuS6pe1\ndrExZqkxZjEQA/p4XZOHrgYOARaWez5cZ61d5V1JUpW0b0mLiKSytB2CJyKSDhTSIiI+ppAWEfEx\nhbSIiI8ppEVEfEwhLSLiYwppEREf+//9dj4YEEfhpwAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f7471f945f8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"_PD_ctnrCyk3","colab_type":"text"},"cell_type":"markdown","source":["Implement Leaky ReLU in TensorFlow:"]},{"metadata":{"id":"K2Twu2l4C2TD","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GR7qnraNC_ly","colab_type":"code","colab":{}},"cell_type":"code","source":["def leaky_relu(z, name=None):\n","  return tf.maximum(0.01*z, z, name=name)\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name='hidden1')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0_sMbWp3DSBg","colab_type":"text"},"cell_type":"markdown","source":["Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"]},{"metadata":{"id":"NaY6HElhDVAS","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AtD9_GdmDWk8","colab_type":"code","colab":{}},"cell_type":"code","source":["X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fPvTvW2MDYRF","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sV4p-hCJDa6-","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V3vLNsseDcir","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kiHDMPrqDezz","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"golILYqTDhOb","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LCRYFVzCDjXa","colab_type":"text"},"cell_type":"markdown","source":["Load the data,"]},{"metadata":{"id":"I8PGL6u_DmOD","colab_type":"code","colab":{}},"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n","X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","X_valid, X_train = X_train[:5000], X_train[5000:]\n","y_valid, y_train = y_train[:5000], y_train[5000:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DN1c2yqJDsIT","colab_type":"code","colab":{}},"cell_type":"code","source":["def shuffle_batch(X, y, batch_size):\n","  rnd_idx = np.random.permutation(len(X))\n","  n_batches = len(X) // batch_size\n","  \n","  for batch_idx in np.array_split(rnd_idx, n_batches):\n","    X_batch, y_batch = X[batch_idx], y[batch_idx]\n","    yield X_batch, y_batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qqtBUBatEDe8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"976e1da5-ea9f-46a0-f7fd-ed99f875c47b","executionInfo":{"status":"ok","timestamp":1540837668186,"user_tz":240,"elapsed":139030,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 40\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","  init.run()\n","  for epoch in range(n_epochs):\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","      sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n","    if epoch % 5 == 0:\n","      acc_batch = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n","      acc_valid = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n","      print(epoch, 'Batch accuracy:', acc_batch, 'Validation accuracy:', acc_valid)\n","      \n","  save_path = saver.save(sess, './my_model_final.ckpt')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["0 Batch accuracy: 0.86 Validation accuracy: 0.86\n","5 Batch accuracy: 0.94 Validation accuracy: 0.94\n","10 Batch accuracy: 0.92 Validation accuracy: 0.92\n","15 Batch accuracy: 0.94 Validation accuracy: 0.94\n","20 Batch accuracy: 1.0 Validation accuracy: 1.0\n","25 Batch accuracy: 1.0 Validation accuracy: 1.0\n","30 Batch accuracy: 0.98 Validation accuracy: 0.98\n","35 Batch accuracy: 1.0 Validation accuracy: 1.0\n"],"name":"stdout"}]},{"metadata":{"id":"1p2wyYXiEzCj","colab_type":"text"},"cell_type":"markdown","source":["###Exponential lineaar unit (ELU)\n","outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set.\n","\n","$ELU_{\\alpha} = \n","\\left\\{\n","        \\begin{array}{ll}\n","            \\alpha (\\exp(z)-1) & \\quad z < 0 \\\\\n","            z & \\quad z \\geq 0\n","        \\end{array}\n","\\right.$"]},{"metadata":{"id":"YMoTzvs9F1qB","colab_type":"code","colab":{}},"cell_type":"code","source":["def elu(z, alpha=1):\n","  return np.where(z<0, alpha*(np.exp(z)-1), z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sq3ar0JsF1jv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":301},"outputId":"3c78f41a-9ee4-493c-f4b9-e14b98ce524a","executionInfo":{"status":"ok","timestamp":1540837884742,"user_tz":240,"elapsed":708,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["plt.plot(z, elu(z), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [-1, -1], 'k--')\n","plt.plot([0, 0], [-2.2, 3.2], 'k-')\n","plt.grid(True)\n","plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n","plt.axis([-5, 5, -2.2, 3.2])"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-5, 5, -2.2, 3.2]"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW8AAAELCAYAAAAWWQdYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny8YAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f74787a5a58>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"xJ_r4zI9GG4Z","colab_type":"text"},"cell_type":"markdown","source":["Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"]},{"metadata":{"id":"WzFXQ7OJGHYp","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","# change activation to elu\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JDaplZnHGUaX","colab_type":"text"},"cell_type":"markdown","source":["Although your mileage will vary, in general ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If you care a lot about runtime performance, then you may prefer leaky ReLUs over ELUs. If you don’t want to tweak yet another hyperparameter, you may just use the default α values suggested earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set."]},{"metadata":{"id":"XewwTNEJGatR","colab_type":"text"},"cell_type":"markdown","source":["###SELU\n","This activation function was proposed in this great paper by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed of a stack of dense layers using the SELU activation function will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets."]},{"metadata":{"id":"Qoxw7nhNGmxz","colab_type":"code","colab":{}},"cell_type":"code","source":["def selu(z,\n","         scale=1.0507009873554804934193349852946,\n","         alpha=1.6732632423543772848170429916717):\n","    return scale * elu(z, alpha)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wsU5U1j_Gn_1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"3afde67a-db68-4153-9ad1-8266eb75eaa3","executionInfo":{"status":"ok","timestamp":1540838040533,"user_tz":240,"elapsed":553,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["plt.plot(z, selu(z), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n","plt.plot([0, 0], [-2.2, 3.2], 'k-')\n","plt.grid(True)\n","plt.title(r\"SELU activation function\", fontsize=14)\n","plt.axis([-5, 5, -2.2, 3.2])"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-5, 5, -2.2, 3.2]"]},"metadata":{"tags":[]},"execution_count":26},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW8AAAEJCAYAAABbkaZTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbjjcCaYAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f747f00cbe0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"9pNvWXNmGrAx","colab_type":"text"},"cell_type":"markdown","source":["By default, the SELU hyperparameters (**scale** and **alpha**) are tuned in such a way that the mean remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"]},{"metadata":{"id":"zbf08_VEGqFY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"87f2a9bd-5110-4edd-d16e-8f47d15a1b0a","executionInfo":{"status":"ok","timestamp":1540838113763,"user_tz":240,"elapsed":737,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["np.random.seed(42)\n","Z = np.random.normal(size=(500, 100))\n","for layer in range(100):\n","    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n","    Z = selu(np.dot(Z, W))\n","    means = np.mean(Z, axis=1)\n","    stds = np.std(Z, axis=1)\n","    if layer % 10 == 0:\n","        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n","            layer, means.min(), means.max(), stds.min(), stds.max()))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n","Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n","Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n","Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n","Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n","Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n","Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n","Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n","Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n","Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"],"name":"stdout"}]},{"metadata":{"id":"AjQBt5g7G9si","colab_type":"text"},"cell_type":"markdown","source":["The **tf.nn.selu()** function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:"]},{"metadata":{"id":"F_dy3wSLHE-9","colab_type":"code","colab":{}},"cell_type":"code","source":["def selu(z,\n","         scale=1.0507009873554804934193349852946,\n","         alpha=1.6732632423543772848170429916717):\n","    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HCeL-BRaHGto","colab_type":"text"},"cell_type":"markdown","source":["However, the SELU activation function cannot be used along with regular Dropout (this would cancel the SELU activation function's self-normalizing property). Fortunately, there is a Dropout variant called Alpha Dropout proposed in the same paper. It is available in **tf.contrib.nn.alpha_dropout()** since TF 1.4\n","\n","Create a neural net for MNIST using the SELU activation function:"]},{"metadata":{"id":"VWF9-vmcHS9U","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JNNVKB41Hgbb","colab_type":"text"},"cell_type":"markdown","source":["Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"]},{"metadata":{"id":"lZBTtyItHg-w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"61d735b6-2239-4496-ed3d-8c47c6237786","executionInfo":{"status":"ok","timestamp":1540838448358,"user_tz":240,"elapsed":142137,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","n_epochs = 40\n","batch_size = 50\n","\n","means = X_train.mean(axis=0, keepdims=True)\n","stds = X_train.std(axis=0, keepdims=True) + 1e-10\n","X_val_scaled = (X_valid - means) / stds\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            X_batch_scaled = (X_batch - means) / stds\n","            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n","        if epoch % 5 == 0:\n","            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n","            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n","            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","\n","    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["0 Batch accuracy: 0.88 Validation accuracy: 0.9232\n","5 Batch accuracy: 0.98 Validation accuracy: 0.9572\n","10 Batch accuracy: 1.0 Validation accuracy: 0.9662\n","15 Batch accuracy: 0.96 Validation accuracy: 0.9682\n","20 Batch accuracy: 1.0 Validation accuracy: 0.9692\n","25 Batch accuracy: 1.0 Validation accuracy: 0.9688\n","30 Batch accuracy: 1.0 Validation accuracy: 0.9692\n","35 Batch accuracy: 1.0 Validation accuracy: 0.97\n"],"name":"stdout"}]},{"metadata":{"id":"05hV_5WZI3Ii","colab_type":"text"},"cell_type":"markdown","source":["##Batch Normalization\n","consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting).\n","\n","*Batch Normalization algorithm\n","$\n","1. \\mu_B = \\frac{1}{m_B}\\sum\\limits_{i=1}^{m_B} x^{(i)} \\\\\n","2. \\sigma_B = \\frac{1}{m_B} \\sum\\limits_{i=1}^{m_B} (x^{(i)}-\\mu_B)^2 \\\\\n","3. \\hat{x}^{(i)} = \\frac{x^{(i)}-\\mu_B}{\\sqrt{\\mu_B^2+ \\epsilon}} \\\\\n","4. z^{(i)}= \\gamma \\hat{x^{(i)}} + \\beta\n","$\n","where\n","* $\\mu_B$ is the empirical mean, evaluated over the whole mini-batch B.\n","* $\\sigma_B$ is the empirical standard deviation, also evaluated over the whole mini-batch.\n","* $m_B$ is the number of instances in the mini-batch.\n","* $\\hat{x^{(i)}}$ is the zero-centered and normalized input.\n","* $\\gamma$ is the scaling parameter for the layer.\n","* $\\epsilon$ is a tiny number to avoid division by zero (typically $10^{–3}$). This is called a *smoothing term*.\n","* $z^{(i)}$ is the output of the BN operation: it is a scaled and shifted version of the inputs."]},{"metadata":{"id":"Tyin6pNaR5ta","colab_type":"text"},"cell_type":"markdown","source":["At test time, there is no mini-batch to compute the empirical mean and standard deviation, so instead you simply use the whole training set’s mean and standard deviation. So, in total, four parameters are learned for each batch-normalized layer: $\\gamma$(scale), $\\beta$ (offset), $\\mu$(mean), and $\\sigma$(standard deviation).\n","\n","Batch Normalization does, however, add some complexity to the model. The neural network makes slower predictions due to the extra computations required at each layer. So if you need predictions to be lightning-fast, you may want to check how well plain ELU + He initialization perform before playing with Batch Normalization."]},{"metadata":{"id":"yhUg0rHQSXVa","colab_type":"text"},"cell_type":"markdown","source":["###Implementing Batch Normalization with TensorFlow"]},{"metadata":{"id":"a9BkqGNDTBsW","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","import tensorflow as tf\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","# This tells batch_normalization if it should use the current minibatch mean and std or\n","# the running averages that it keeps track of\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n","bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n","bn1_act = tf.nn.elu(bn1)\n","\n","hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n","bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n","bn2_act = tf.nn.elu(bn2)\n","\n","logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n","logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZWsTHhUBVPGL","colab_type":"text"},"cell_type":"markdown","source":["To avoid repeating the same parameters over and over again, we can use Python's partial() function:"]},{"metadata":{"id":"bJyUg6ywVej8","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","training = tf.placeholder_with_default(False, shape=(), name='training')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HY4SpIlVgXt","colab_type":"code","colab":{}},"cell_type":"code","source":["from functools import partial\n","\n","my_batch_norm_layer = partial(tf.layers.batch_normalization,\n","                              training=training, momentum=0.9)\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n","bn1 = my_batch_norm_layer(hidden1)\n","bn1_act = tf.nn.elu(bn1)\n","hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n","bn2 = my_batch_norm_layer(hidden2)\n","bn2_act = tf.nn.elu(bn2)\n","logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n","logits = my_batch_norm_layer(logits_before_bn)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n8R7BQrlVstq","colab_type":"text"},"cell_type":"markdown","source":["Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"]},{"metadata":{"id":"ipOm8ebvVtcc","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","batch_norm_momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","training = tf.placeholder_with_default(False, shape=(), name='training')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Rg2p5dqVy9N","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope('dnn'):\n","  he_init = tf.variance_scaling_initializer()\n","  \n","  my_batch_norm_layer = partial(tf.layers.batch_normalization,\n","                               training=training,\n","                               momentum=batch_norm_momentum)\n","  \n","  my_dense_layer = partial(tf.layers.dense,\n","                          kernel_initializer=he_init)\n","  \n","  hidden1 = my_dense_layer(X, n_hidden1, name='hidden1')\n","  bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n","  hidden2 = my_dense_layer(hidden1, n_hidden2, name='hidden2')\n","  bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n","  logits_before_bn = my_dense_layer(bn2, n_outputs, name='outputs')\n","  logits = my_batch_norm_layer(logits_before_bn)\n","  \n","with tf.name_scope('loss'):\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","  loss = tf.reduce_mean(xentropy, name='loss')\n","  \n","with tf.name_scope('train'):\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","  training_op = optimizer.minimize(loss)\n","  \n","with tf.name_scope('eval'):\n","  correct = tf.nn.in_top_k(logits, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ao_blM8FXdP5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"cbc24a52-c1a9-4f1e-e7ab-9712dcef17ea","executionInfo":{"status":"ok","timestamp":1540842735410,"user_tz":240,"elapsed":57993,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200\n","\n","extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","\n","with tf.Session() as sess:\n","  init.run()\n","  \n","  for epoch in range(n_epochs):\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","      sess.run([training_op, extra_update_ops],\n","              feed_dict={training:True, X:X_batch, y:y_batch})\n","    accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n","    print(epoch, 'Validation accuracy:', accuracy_val)\n","      \n","  save_path = saver.save(sess, './my_model_final.ckpt')"],"execution_count":43,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.883\n","1 Validation accuracy: 0.9112\n","2 Validation accuracy: 0.9212\n","3 Validation accuracy: 0.9284\n","4 Validation accuracy: 0.9358\n","5 Validation accuracy: 0.9426\n","6 Validation accuracy: 0.9494\n","7 Validation accuracy: 0.9508\n","8 Validation accuracy: 0.9546\n","9 Validation accuracy: 0.9576\n","10 Validation accuracy: 0.9596\n","11 Validation accuracy: 0.9622\n","12 Validation accuracy: 0.9648\n","13 Validation accuracy: 0.966\n","14 Validation accuracy: 0.9682\n","15 Validation accuracy: 0.9672\n","16 Validation accuracy: 0.9682\n","17 Validation accuracy: 0.9704\n","18 Validation accuracy: 0.9694\n","19 Validation accuracy: 0.9684\n"],"name":"stdout"}]},{"metadata":{"id":"DV0_kBx2YaOi","colab_type":"text"},"cell_type":"markdown","source":["Note that you could also make the training operation depend on the update operations:"]},{"metadata":{"id":"nlYVN_1rYiEp","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","    with tf.control_dependencies(extra_update_ops):\n","        training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9Q0b6Qa5Ynxi","colab_type":"text"},"cell_type":"markdown","source":["This way, you would just have to evaluate the training_op during training, TensorFlow would automatically run the update operations as well:"]},{"metadata":{"id":"uqWvGPooYqZH","colab_type":"code","colab":{}},"cell_type":"code","source":["sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RwHWywfmYvmP","colab_type":"text"},"cell_type":"markdown","source":["One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."]},{"metadata":{"id":"hGHrGr_aYwyx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"5707b739-2a16-489b-deb8-88f7fad3faf1","executionInfo":{"status":"ok","timestamp":1540842793093,"user_tz":240,"elapsed":393,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["[v.name for v in tf.trainable_variables()]"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hidden1/kernel:0',\n"," 'hidden1/bias:0',\n"," 'batch_normalization/gamma:0',\n"," 'batch_normalization/beta:0',\n"," 'hidden2/kernel:0',\n"," 'hidden2/bias:0',\n"," 'batch_normalization_1/gamma:0',\n"," 'batch_normalization_1/beta:0',\n"," 'outputs/kernel:0',\n"," 'outputs/bias:0',\n"," 'batch_normalization_2/gamma:0',\n"," 'batch_normalization_2/beta:0']"]},"metadata":{"tags":[]},"execution_count":44}]},{"metadata":{"id":"JoX0iOL8Yz7l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"af455bee-d688-475a-af0e-8a13ad43d4c6","executionInfo":{"status":"ok","timestamp":1540842801046,"user_tz":240,"elapsed":412,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["[v.name for v in tf.global_variables()]"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hidden1/kernel:0',\n"," 'hidden1/bias:0',\n"," 'batch_normalization/gamma:0',\n"," 'batch_normalization/beta:0',\n"," 'batch_normalization/moving_mean:0',\n"," 'batch_normalization/moving_variance:0',\n"," 'hidden2/kernel:0',\n"," 'hidden2/bias:0',\n"," 'batch_normalization_1/gamma:0',\n"," 'batch_normalization_1/beta:0',\n"," 'batch_normalization_1/moving_mean:0',\n"," 'batch_normalization_1/moving_variance:0',\n"," 'outputs/kernel:0',\n"," 'outputs/bias:0',\n"," 'batch_normalization_2/gamma:0',\n"," 'batch_normalization_2/beta:0',\n"," 'batch_normalization_2/moving_mean:0',\n"," 'batch_normalization_2/moving_variance:0']"]},"metadata":{"tags":[]},"execution_count":45}]},{"metadata":{"id":"5hrlMBKSZD0K","colab_type":"text"},"cell_type":"markdown","source":["##Gradient Clipping\n","A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural networks.)"]},{"metadata":{"id":"Zeft0B3nZmvE","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_hidden3 = 50\n","n_hidden4 = 50\n","n_hidden5 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zn5UigAmZonT","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n","    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n","    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i1me3H1bZtxH","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rX78Sxw2Zuj6","colab_type":"text"},"cell_type":"markdown","source":["Now we apply gradient clipping. For this, we need to get the gradients, use the clip_by_value() function to clip them, then apply them:"]},{"metadata":{"id":"aGt7HMVmZsBm","colab_type":"code","colab":{}},"cell_type":"code","source":["threshold = 1.0\n","\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","grads_and_vars = optimizer.compute_gradients(loss)\n","capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n","training_op = optimizer.apply_gradients(capped_gvs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8au-1ya1aKN1","colab_type":"text"},"cell_type":"markdown","source":["The rest is the same....."]},{"metadata":{"id":"KJUiimHmaM3F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"2068417a-7497-43ab-866e-e97e7b359231","executionInfo":{"status":"ok","timestamp":1540843241540,"user_tz":240,"elapsed":45729,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs = 20\n","batch_size = 200\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":50,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.2876\n","1 Validation accuracy: 0.7934\n","2 Validation accuracy: 0.8798\n","3 Validation accuracy: 0.9064\n","4 Validation accuracy: 0.9162\n","5 Validation accuracy: 0.9222\n","6 Validation accuracy: 0.9292\n","7 Validation accuracy: 0.9358\n","8 Validation accuracy: 0.9382\n","9 Validation accuracy: 0.9414\n","10 Validation accuracy: 0.9458\n","11 Validation accuracy: 0.9472\n","12 Validation accuracy: 0.9476\n","13 Validation accuracy: 0.9536\n","14 Validation accuracy: 0.9566\n","15 Validation accuracy: 0.9566\n","16 Validation accuracy: 0.9576\n","17 Validation accuracy: 0.9588\n","18 Validation accuracy: 0.9622\n","19 Validation accuracy: 0.9614\n"],"name":"stdout"}]},{"metadata":{"id":"D5nwx_ewaky2","colab_type":"text"},"cell_type":"markdown","source":["#Reusing Pretrained Layers\n","Transfer learning: It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network.\n","\n","If the input pictures of your new task don’t have the same size as the ones used in the original task, you will have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work only well if the\n","inputs have similar low-level features."]},{"metadata":{"id":"SItw0lNxbCoy","colab_type":"text"},"cell_type":"markdown","source":["##Reusing a TensorFlow Model\n","First you need to load the graph's structure. The **import_meta_graph()** function does just that, loading the graph's operations into the default graph, and returning a **Saver** that you can then use to restore the model's state. Note that by default, a **Saver** saves the structure of the graph into a **.meta** file, so that's the file you should load:"]},{"metadata":{"id":"zgN-4Vl_ncjb","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ElsEJTPqneFN","colab_type":"code","colab":{}},"cell_type":"code","source":["saver = tf.train.import_meta_graph('./my_model_final.ckpt.meta')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A8apATNEnkeb","colab_type":"text"},"cell_type":"markdown","source":["Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations"]},{"metadata":{"id":"wV4B8zRJnpSN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4763},"outputId":"24132bec-58ed-472e-86a6-23366634721c","executionInfo":{"status":"ok","timestamp":1540846706894,"user_tz":240,"elapsed":384,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["for op in tf.get_default_graph().get_operations():\n","  print(op.name)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["X\n","y\n","hidden1/kernel/Initializer/random_uniform/shape\n","hidden1/kernel/Initializer/random_uniform/min\n","hidden1/kernel/Initializer/random_uniform/max\n","hidden1/kernel/Initializer/random_uniform/RandomUniform\n","hidden1/kernel/Initializer/random_uniform/sub\n","hidden1/kernel/Initializer/random_uniform/mul\n","hidden1/kernel/Initializer/random_uniform\n","hidden1/kernel\n","hidden1/kernel/Assign\n","hidden1/kernel/read\n","hidden1/bias/Initializer/zeros\n","hidden1/bias\n","hidden1/bias/Assign\n","hidden1/bias/read\n","dnn/hidden1/MatMul\n","dnn/hidden1/BiasAdd\n","dnn/hidden1/Relu\n","hidden2/kernel/Initializer/random_uniform/shape\n","hidden2/kernel/Initializer/random_uniform/min\n","hidden2/kernel/Initializer/random_uniform/max\n","hidden2/kernel/Initializer/random_uniform/RandomUniform\n","hidden2/kernel/Initializer/random_uniform/sub\n","hidden2/kernel/Initializer/random_uniform/mul\n","hidden2/kernel/Initializer/random_uniform\n","hidden2/kernel\n","hidden2/kernel/Assign\n","hidden2/kernel/read\n","hidden2/bias/Initializer/zeros\n","hidden2/bias\n","hidden2/bias/Assign\n","hidden2/bias/read\n","dnn/hidden2/MatMul\n","dnn/hidden2/BiasAdd\n","dnn/hidden2/Relu\n","hidden3/kernel/Initializer/random_uniform/shape\n","hidden3/kernel/Initializer/random_uniform/min\n","hidden3/kernel/Initializer/random_uniform/max\n","hidden3/kernel/Initializer/random_uniform/RandomUniform\n","hidden3/kernel/Initializer/random_uniform/sub\n","hidden3/kernel/Initializer/random_uniform/mul\n","hidden3/kernel/Initializer/random_uniform\n","hidden3/kernel\n","hidden3/kernel/Assign\n","hidden3/kernel/read\n","hidden3/bias/Initializer/zeros\n","hidden3/bias\n","hidden3/bias/Assign\n","hidden3/bias/read\n","dnn/hidden3/MatMul\n","dnn/hidden3/BiasAdd\n","dnn/hidden3/Relu\n","hidden4/kernel/Initializer/random_uniform/shape\n","hidden4/kernel/Initializer/random_uniform/min\n","hidden4/kernel/Initializer/random_uniform/max\n","hidden4/kernel/Initializer/random_uniform/RandomUniform\n","hidden4/kernel/Initializer/random_uniform/sub\n","hidden4/kernel/Initializer/random_uniform/mul\n","hidden4/kernel/Initializer/random_uniform\n","hidden4/kernel\n","hidden4/kernel/Assign\n","hidden4/kernel/read\n","hidden4/bias/Initializer/zeros\n","hidden4/bias\n","hidden4/bias/Assign\n","hidden4/bias/read\n","dnn/hidden4/MatMul\n","dnn/hidden4/BiasAdd\n","dnn/hidden4/Relu\n","hidden5/kernel/Initializer/random_uniform/shape\n","hidden5/kernel/Initializer/random_uniform/min\n","hidden5/kernel/Initializer/random_uniform/max\n","hidden5/kernel/Initializer/random_uniform/RandomUniform\n","hidden5/kernel/Initializer/random_uniform/sub\n","hidden5/kernel/Initializer/random_uniform/mul\n","hidden5/kernel/Initializer/random_uniform\n","hidden5/kernel\n","hidden5/kernel/Assign\n","hidden5/kernel/read\n","hidden5/bias/Initializer/zeros\n","hidden5/bias\n","hidden5/bias/Assign\n","hidden5/bias/read\n","dnn/hidden5/MatMul\n","dnn/hidden5/BiasAdd\n","dnn/hidden5/Relu\n","outputs/kernel/Initializer/random_uniform/shape\n","outputs/kernel/Initializer/random_uniform/min\n","outputs/kernel/Initializer/random_uniform/max\n","outputs/kernel/Initializer/random_uniform/RandomUniform\n","outputs/kernel/Initializer/random_uniform/sub\n","outputs/kernel/Initializer/random_uniform/mul\n","outputs/kernel/Initializer/random_uniform\n","outputs/kernel\n","outputs/kernel/Assign\n","outputs/kernel/read\n","outputs/bias/Initializer/zeros\n","outputs/bias\n","outputs/bias/Assign\n","outputs/bias/read\n","dnn/outputs/MatMul\n","dnn/outputs/BiasAdd\n","loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n","loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n","loss/Const\n","loss/loss\n","gradients/Shape\n","gradients/grad_ys_0\n","gradients/Fill\n","gradients/loss/loss_grad/Reshape/shape\n","gradients/loss/loss_grad/Reshape\n","gradients/loss/loss_grad/Shape\n","gradients/loss/loss_grad/Tile\n","gradients/loss/loss_grad/Shape_1\n","gradients/loss/loss_grad/Shape_2\n","gradients/loss/loss_grad/Const\n","gradients/loss/loss_grad/Prod\n","gradients/loss/loss_grad/Const_1\n","gradients/loss/loss_grad/Prod_1\n","gradients/loss/loss_grad/Maximum/y\n","gradients/loss/loss_grad/Maximum\n","gradients/loss/loss_grad/floordiv\n","gradients/loss/loss_grad/Cast\n","gradients/loss/loss_grad/truediv\n","gradients/zeros_like\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n","gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/outputs/MatMul_grad/MatMul\n","gradients/dnn/outputs/MatMul_grad/MatMul_1\n","gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n","gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n","gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden5/Relu_grad/ReluGrad\n","gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden5/MatMul_grad/MatMul\n","gradients/dnn/hidden5/MatMul_grad/MatMul_1\n","gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden4/Relu_grad/ReluGrad\n","gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden4/MatMul_grad/MatMul\n","gradients/dnn/hidden4/MatMul_grad/MatMul_1\n","gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden3/Relu_grad/ReluGrad\n","gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden3/MatMul_grad/MatMul\n","gradients/dnn/hidden3/MatMul_grad/MatMul_1\n","gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden2/Relu_grad/ReluGrad\n","gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden2/MatMul_grad/MatMul\n","gradients/dnn/hidden2/MatMul_grad/MatMul_1\n","gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden1/Relu_grad/ReluGrad\n","gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden1/MatMul_grad/MatMul\n","gradients/dnn/hidden1/MatMul_grad/MatMul_1\n","gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n","clip_by_value/Minimum/y\n","clip_by_value/Minimum\n","clip_by_value/y\n","clip_by_value\n","clip_by_value_1/Minimum/y\n","clip_by_value_1/Minimum\n","clip_by_value_1/y\n","clip_by_value_1\n","clip_by_value_2/Minimum/y\n","clip_by_value_2/Minimum\n","clip_by_value_2/y\n","clip_by_value_2\n","clip_by_value_3/Minimum/y\n","clip_by_value_3/Minimum\n","clip_by_value_3/y\n","clip_by_value_3\n","clip_by_value_4/Minimum/y\n","clip_by_value_4/Minimum\n","clip_by_value_4/y\n","clip_by_value_4\n","clip_by_value_5/Minimum/y\n","clip_by_value_5/Minimum\n","clip_by_value_5/y\n","clip_by_value_5\n","clip_by_value_6/Minimum/y\n","clip_by_value_6/Minimum\n","clip_by_value_6/y\n","clip_by_value_6\n","clip_by_value_7/Minimum/y\n","clip_by_value_7/Minimum\n","clip_by_value_7/y\n","clip_by_value_7\n","clip_by_value_8/Minimum/y\n","clip_by_value_8/Minimum\n","clip_by_value_8/y\n","clip_by_value_8\n","clip_by_value_9/Minimum/y\n","clip_by_value_9/Minimum\n","clip_by_value_9/y\n","clip_by_value_9\n","clip_by_value_10/Minimum/y\n","clip_by_value_10/Minimum\n","clip_by_value_10/y\n","clip_by_value_10\n","clip_by_value_11/Minimum/y\n","clip_by_value_11/Minimum\n","clip_by_value_11/y\n","clip_by_value_11\n","GradientDescent/learning_rate\n","GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden1/bias/ApplyGradientDescent\n","GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden2/bias/ApplyGradientDescent\n","GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden3/bias/ApplyGradientDescent\n","GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden4/bias/ApplyGradientDescent\n","GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden5/bias/ApplyGradientDescent\n","GradientDescent/update_outputs/kernel/ApplyGradientDescent\n","GradientDescent/update_outputs/bias/ApplyGradientDescent\n","GradientDescent\n","eval/in_top_k/InTopKV2/k\n","eval/in_top_k/InTopKV2\n","eval/Cast\n","eval/Const\n","eval/accuracy\n","init\n","save/Const\n","save/SaveV2/tensor_names\n","save/SaveV2/shape_and_slices\n","save/SaveV2\n","save/control_dependency\n","save/RestoreV2/tensor_names\n","save/RestoreV2/shape_and_slices\n","save/RestoreV2\n","save/Assign\n","save/Assign_1\n","save/Assign_2\n","save/Assign_3\n","save/Assign_4\n","save/Assign_5\n","save/Assign_6\n","save/Assign_7\n","save/Assign_8\n","save/Assign_9\n","save/Assign_10\n","save/Assign_11\n","save/restore_all\n"],"name":"stdout"}]},{"metadata":{"id":"xT88gW2Rnx4_","colab_type":"text"},"cell_type":"markdown","source":["That's.... a lot..\n","\n","It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a FileWriter to save the graph and then visualize it in TensorBoard):"]},{"metadata":{"id":"pbXP5SX9n6UC","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorflow_graph_in_jupyter import show_graph\n","show_graph(tf.get_default_graph())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EWWUAlo7n9xi","colab_type":"text"},"cell_type":"markdown","source":["Once you know which operations you need, you can get a handle on them using the graph's **get_operation_by_name()** or **get_tensor_by_name()** methods:"]},{"metadata":{"id":"igO2evoMoCEE","colab_type":"code","colab":{}},"cell_type":"code","source":["X = tf.get_default_graph().get_tensor_by_name('X:0')\n","y = tf.get_default_graph().get_tensor_by_name('y:0')\n","\n","accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy:0')\n","\n","training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pc_DTdC-ob7x","colab_type":"text"},"cell_type":"markdown","source":["If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. \n","\n","Another approach is to create a collection containing all the important operations that people will want to get a handle on:"]},{"metadata":{"id":"vGjIdCLZrzbF","colab_type":"code","colab":{}},"cell_type":"code","source":["for op in (X, y, accuracy, training_op):\n","  tf.add_to_collection('my_important_ops', op)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z-UG32O6r9FK","colab_type":"text"},"cell_type":"markdown","source":["In this way people who reuse your model will be able to simply write:"]},{"metadata":{"id":"ZKtkd4upsAhk","colab_type":"code","colab":{}},"cell_type":"code","source":["X, y, accuracy, training_op = tf.get_collection('my_important_ops')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5omethQsGak","colab_type":"text"},"cell_type":"markdown","source":["start a session, restore the model's state and continue training on your data:"]},{"metadata":{"id":"-H6UvtkisImy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"c16831a1-0e8c-43c8-c5bb-5a897553bc98","executionInfo":{"status":"ok","timestamp":1540847991399,"user_tz":240,"elapsed":49433,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  saver.restore(sess, './my_model_final.ckpt')\n","  \n","  for epoch in range(n_epochs):\n","      for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","          sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","      accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","      print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","  save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":60,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.9642\n","1 Validation accuracy: 0.9632\n","2 Validation accuracy: 0.9654\n","3 Validation accuracy: 0.9652\n","4 Validation accuracy: 0.9642\n","5 Validation accuracy: 0.965\n","6 Validation accuracy: 0.9684\n","7 Validation accuracy: 0.9686\n","8 Validation accuracy: 0.9686\n","9 Validation accuracy: 0.9688\n","10 Validation accuracy: 0.9702\n","11 Validation accuracy: 0.9716\n","12 Validation accuracy: 0.967\n","13 Validation accuracy: 0.9702\n","14 Validation accuracy: 0.9714\n","15 Validation accuracy: 0.9722\n","16 Validation accuracy: 0.9722\n","17 Validation accuracy: 0.971\n","18 Validation accuracy: 0.9712\n","19 Validation accuracy: 0.9716\n"],"name":"stdout"}]},{"metadata":{"id":"9hfPk4THsirm","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, if you have access to the Python code that built the original graph, you can use it instead of **import_meta_graph()**:"]},{"metadata":{"id":"tgw2BhqXsmYb","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_hidden3 = 50\n","n_hidden4 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n","    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n","    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","threshold = 1.0\n","\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","grads_and_vars = optimizer.compute_gradients(loss)\n","capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n","              for grad, var in grads_and_vars]\n","training_op = optimizer.apply_gradients(capped_gvs)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mmiP8UZMsrmc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"3697b2b2-b702-4600-f525-b23582e70904","executionInfo":{"status":"ok","timestamp":1540848082591,"user_tz":240,"elapsed":47021,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":62,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.9642\n","1 Validation accuracy: 0.9632\n","2 Validation accuracy: 0.9654\n","3 Validation accuracy: 0.9652\n","4 Validation accuracy: 0.9642\n","5 Validation accuracy: 0.965\n","6 Validation accuracy: 0.9684\n","7 Validation accuracy: 0.9686\n","8 Validation accuracy: 0.9686\n","9 Validation accuracy: 0.9688\n","10 Validation accuracy: 0.9702\n","11 Validation accuracy: 0.9716\n","12 Validation accuracy: 0.967\n","13 Validation accuracy: 0.9702\n","14 Validation accuracy: 0.9714\n","15 Validation accuracy: 0.9722\n","16 Validation accuracy: 0.9722\n","17 Validation accuracy: 0.971\n","18 Validation accuracy: 0.9712\n","19 Validation accuracy: 0.9716\n"],"name":"stdout"}]},{"metadata":{"id":"7b7CNaC_s1JE","colab_type":"text"},"cell_type":"markdown","source":["In general you will want to reuse only the lower layers. If you are using **import_meta_graph()** it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we *add a new 4th hidden layer on top of the pretrained 3rd layer* (ignoring the old 4th hidden layer). We also build a *new output layer*, *the loss for this new output*, and a *new optimizer to minimize it*. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"]},{"metadata":{"id":"DBl01zo3tMKw","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_hidden4 = 20  # new layer\n","n_outputs = 10  # new layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LsyFtYjTtN-3","colab_type":"code","colab":{}},"cell_type":"code","source":["saver = tf.train.import_meta_graph('./my_model_final.ckpt.meta')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z_32ej-WtSGA","colab_type":"code","colab":{}},"cell_type":"code","source":["# reuse the variables\n","X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n","y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n","\n","hidden3 =tf.get_default_graph().get_tensor_by_name('dnn/hidden2/Relu:0')\n","\n","new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='new_hidden4')\n","new_logits = tf.layers.dense(new_hidden4, n_outputs, name='new_outputs')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ihrWiMAgt0TS","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope('new_loss'):\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n","  loss = tf.reduce_mean(xentropy, name='loss')\n","  \n","with tf.name_scope('new_eval'):\n","  correct = tf.nn.in_top_k(new_logits, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n","  \n","with tf.name_scope('new_train'):\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","  training_op = optimizer.minimize(loss)\n","  \n","init = tf.global_variables_initializer()\n","new_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ixhiWOmcudGu","colab_type":"text"},"cell_type":"markdown","source":["And we can train this new model:"]},{"metadata":{"id":"eLLjB3NKuesU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"93484fc7-0374-4c60-d9ac-c0842c6f69cc","executionInfo":{"status":"ok","timestamp":1540848687827,"user_tz":240,"elapsed":46347,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  init.run()\n","  saver.restore(sess, './my_model_final.ckpt')\n","  \n","  for epoch in range(n_epochs):\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","    print(epoch, 'Validation accuracy:', accuracy_val)\n","  \n","  save_path = new_saver.save(sess, './my_new_model_final.ckpt')"],"execution_count":67,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.8888\n","1 Validation accuracy: 0.919\n","2 Validation accuracy: 0.9318\n","3 Validation accuracy: 0.939\n","4 Validation accuracy: 0.9438\n","5 Validation accuracy: 0.9446\n","6 Validation accuracy: 0.9482\n","7 Validation accuracy: 0.9516\n","8 Validation accuracy: 0.9532\n","9 Validation accuracy: 0.9534\n","10 Validation accuracy: 0.9568\n","11 Validation accuracy: 0.9572\n","12 Validation accuracy: 0.9578\n","13 Validation accuracy: 0.9594\n","14 Validation accuracy: 0.9616\n","15 Validation accuracy: 0.9618\n","16 Validation accuracy: 0.9624\n","17 Validation accuracy: 0.9628\n","18 Validation accuracy: 0.9638\n","19 Validation accuracy: 0.9644\n"],"name":"stdout"}]},{"metadata":{"id":"sRvDWVVwvMCf","colab_type":"text"},"cell_type":"markdown","source":["If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"]},{"metadata":{"id":"LVscjPfkvPBR","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sUBxHyyrvVB1","colab_type":"text"},"cell_type":"markdown","source":["However, you must create one Saver to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another Saver to save the new model, once it is trained:"]},{"metadata":{"id":"reufe85NvYiT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"ed6b60ad-0afc-4e69-bac4-4e9166c18957","executionInfo":{"status":"ok","timestamp":1540848806322,"user_tz":240,"elapsed":41122,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val) \n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":70,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.9028\n","1 Validation accuracy: 0.9336\n","2 Validation accuracy: 0.9432\n","3 Validation accuracy: 0.9472\n","4 Validation accuracy: 0.952\n","5 Validation accuracy: 0.9536\n","6 Validation accuracy: 0.9556\n","7 Validation accuracy: 0.959\n","8 Validation accuracy: 0.9588\n","9 Validation accuracy: 0.9604\n","10 Validation accuracy: 0.9624\n","11 Validation accuracy: 0.9618\n","12 Validation accuracy: 0.9642\n","13 Validation accuracy: 0.9662\n","14 Validation accuracy: 0.9664\n","15 Validation accuracy: 0.9658\n","16 Validation accuracy: 0.9672\n","17 Validation accuracy: 0.9672\n","18 Validation accuracy: 0.9682\n","19 Validation accuracy: 0.9674\n"],"name":"stdout"}]},{"metadata":{"id":"SLq3hC8qvw8i","colab_type":"text"},"cell_type":"markdown","source":["##Reusing Models from Other Frameworks\n","In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a **feed_dict**:"]},{"metadata":{"id":"X9WYluCxwM2G","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 2\n","n_hidden1 = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iRMpfVxrwOi3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"213d32be-bf6a-49af-c2ad-df066c1ca4de","executionInfo":{"status":"ok","timestamp":1540848970421,"user_tz":240,"elapsed":360,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n","original_b = [7., 8., 9.]                 # Load the biases from the other framework\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","# [...] Build the rest of the model\n","\n","# Get a handle on the assignment nodes for the hidden1 variables\n","graph = tf.get_default_graph()\n","assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n","assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n","init_kernel = assign_kernel.inputs[1]\n","init_bias = assign_bias.inputs[1]\n","\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n","    # [...] Train the model on your new task\n","    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["[[ 61.  83. 105.]]\n"],"name":"stdout"}]},{"metadata":{"id":"2klammegwooI","colab_type":"text"},"cell_type":"markdown","source":["Another approach would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but more explicit:"]},{"metadata":{"id":"G4laqCGUwwkj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bbdc2012-0174-43de-e9cc-82cd43c0d794","executionInfo":{"status":"ok","timestamp":1540849082429,"user_tz":240,"elapsed":591,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 2\n","n_hidden1 = 3\n","\n","original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n","original_b = [7., 8., 9.]                 # Load the biases from the other framework\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","# [...] Build the rest of the model\n","\n","# Get a handle on the variables of layer hidden1\n","with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n","    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n","    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n","\n","# Create dedicated placeholders and assignment nodes\n","original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n","original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n","assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n","assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n","\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n","    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n","    # [...] Train the model on your new task\n","    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"],"execution_count":73,"outputs":[{"output_type":"stream","text":["[[ 61.  83. 105.]]\n"],"name":"stdout"}]},{"metadata":{"id":"RK1IgYk5wzZh","colab_type":"text"},"cell_type":"markdown","source":["Note that we could also get a handle on the variables using **get_collection()** and specifying the **scope**:"]},{"metadata":{"id":"WbQ8pLqow2fT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6418648b-5ba7-4b42-b547-989a3a8120ab","executionInfo":{"status":"ok","timestamp":1540849107584,"user_tz":240,"elapsed":373,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n"," <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"]},"metadata":{"tags":[]},"execution_count":74}]},{"metadata":{"id":"ky9KrQpkw4j3","colab_type":"text"},"cell_type":"markdown","source":["Or we could use the graph's **get_tensor_by_name()** method:"]},{"metadata":{"id":"xp6Kfd4cw895","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0acb6523-361e-4b5e-b2ce-9ef98a5815bf","executionInfo":{"status":"ok","timestamp":1540849129351,"user_tz":240,"elapsed":383,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"]},"metadata":{"tags":[]},"execution_count":75}]},{"metadata":{"id":"ZSoG0euBw9WU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"672f672d-45ae-4796-b912-e9234402cf93","executionInfo":{"status":"ok","timestamp":1540849135693,"user_tz":240,"elapsed":367,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"]},"metadata":{"tags":[]},"execution_count":76}]},{"metadata":{"id":"FkBLfitFxAVm","colab_type":"text"},"cell_type":"markdown","source":["##Freezing the Lower Layers\n","It is likely that the lower layers of the first DNN have learned to detect low-level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are.\n","\n","It is generally a good idea to “freeze” their weights when training the new DNN: if the lower-layer weights are fixed, then the higher-\n","layer weights will be easier to train.\n","\n","1st DNN,"]},{"metadata":{"id":"johbjXiAxsYk","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OT6c3M_uxsyd","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n","    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n","                                   scope=\"hidden[34]|outputs\")\n","    training_op = optimizer.minimize(loss, var_list=train_vars)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GSV8Dz6Sxv9R","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","new_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vR32wCPhxxSC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"fd73a051-e8e1-4e03-b3cb-b896a17e9c48","executionInfo":{"status":"ok","timestamp":1540849387432,"user_tz":240,"elapsed":22069,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":80,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.8974\n","1 Validation accuracy: 0.9308\n","2 Validation accuracy: 0.9402\n","3 Validation accuracy: 0.9448\n","4 Validation accuracy: 0.9478\n","5 Validation accuracy: 0.951\n","6 Validation accuracy: 0.951\n","7 Validation accuracy: 0.9534\n","8 Validation accuracy: 0.9552\n","9 Validation accuracy: 0.9566\n","10 Validation accuracy: 0.9558\n","11 Validation accuracy: 0.9568\n","12 Validation accuracy: 0.9572\n","13 Validation accuracy: 0.958\n","14 Validation accuracy: 0.9588\n","15 Validation accuracy: 0.958\n","16 Validation accuracy: 0.9576\n","17 Validation accuracy: 0.9602\n","18 Validation accuracy: 0.9592\n","19 Validation accuracy: 0.9602\n"],"name":"stdout"}]},{"metadata":{"id":"s52-14p1yRDh","colab_type":"text"},"cell_type":"markdown","source":["2nd DNN, with frozen layers"]},{"metadata":{"id":"pkcwF3jCx3EM","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h6jRwRmpyCAy","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              name=\"hidden1\") # reused frozen\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              name=\"hidden2\") # reused frozen\n","    hidden2_stop = tf.stop_gradient(hidden2)\n","    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n","                              name=\"hidden3\") # reused, not frozen\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n","                              name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y2RQRqUVyVeD","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KQGjk-Q-yX4t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"abd90b8b-6e4e-4a01-f437-f36071b35f87","executionInfo":{"status":"ok","timestamp":1540849532578,"user_tz":240,"elapsed":21794,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":84,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.902\n","1 Validation accuracy: 0.9304\n","2 Validation accuracy: 0.9436\n","3 Validation accuracy: 0.948\n","4 Validation accuracy: 0.9514\n","5 Validation accuracy: 0.9524\n","6 Validation accuracy: 0.9522\n","7 Validation accuracy: 0.9556\n","8 Validation accuracy: 0.9554\n","9 Validation accuracy: 0.9562\n","10 Validation accuracy: 0.9568\n","11 Validation accuracy: 0.9552\n","12 Validation accuracy: 0.9574\n","13 Validation accuracy: 0.9578\n","14 Validation accuracy: 0.958\n","15 Validation accuracy: 0.957\n","16 Validation accuracy: 0.9564\n","17 Validation accuracy: 0.9574\n","18 Validation accuracy: 0.9594\n","19 Validation accuracy: 0.9578\n"],"name":"stdout"}]},{"metadata":{"id":"QCPE2PMDyorz","colab_type":"text"},"cell_type":"markdown","source":["##Caching the Frozen Layers\n","Since the frozen layers won’t change, it is possible to cache the output of the topmost frozen layer for each training instance.\n","\n","Since training goes through the whole dataset many times, this will give you a huge speed boost as you will only need to go through\n","the frozen layers once per training instance (instead of once per epoch).\n","\n","For example,\n","you could first run the whole training set through the lower layers,"]},{"metadata":{"id":"XpBn63Pny808","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden2_outputs = sess.run(hidden2, feed_dict={X: X_train})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VKVbo5tEzCg7","colab_type":"text"},"cell_type":"markdown","source":["Then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer 2 and feed them to the training operation:"]},{"metadata":{"id":"xFcgSoXMzGBC","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 100\n","n_batches = 500\n","for epoch in range(n_epochs):\n","shuffled_idx = rnd.permutation(len(hidden2_outputs))\n","hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx], n_batches)\n","y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n","for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n","sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sqYwSjiFzHpq","colab_type":"text"},"cell_type":"markdown","source":["To see the complete code,\n"]},{"metadata":{"id":"WspO3UgWzL0e","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              name=\"hidden1\") # reused frozen\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              name=\"hidden2\") # reused frozen & cached\n","    hidden2_stop = tf.stop_gradient(hidden2)\n","    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n","                              name=\"hidden3\") # reused, not frozen\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n","                              name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8zP21xuGzPuq","colab_type":"code","colab":{}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                              scope='hidden[123]')\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dqmAoCJAzkA7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"70be9810-e2d8-4ddc-ebe3-80e0d87c3d70","executionInfo":{"status":"ok","timestamp":1540858936908,"user_tz":240,"elapsed":6202,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_batches = len(X_train) // batch_size\n","\n","with tf.Session() as sess:\n","  init.run()\n","  restore_saver.restore(sess, './my_model_final.ckpt')\n","  \n","  h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n","  h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n","  \n","  for epoch in range(n_epochs):\n","    shuffled_idx = np.random.permutation(len(X_train))\n","    hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n","    y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n","    \n","    for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n","      sess.run(training_op, feed_dict={hidden2: hidden2_batch, y:y_batch})\n","      \n","    accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y:y_valid})\n","    print(epoch, 'Validation accuracy:', accuracy_val)\n","    \n","  save_path = saver.save(sess, './my_new_model_final.ckpt')"],"execution_count":114,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n","0 Validation accuracy: 0.902\n","1 Validation accuracy: 0.9304\n","2 Validation accuracy: 0.9436\n","3 Validation accuracy: 0.948\n","4 Validation accuracy: 0.9514\n","5 Validation accuracy: 0.9524\n","6 Validation accuracy: 0.9522\n","7 Validation accuracy: 0.9556\n","8 Validation accuracy: 0.9554\n","9 Validation accuracy: 0.9562\n","10 Validation accuracy: 0.9568\n","11 Validation accuracy: 0.9552\n","12 Validation accuracy: 0.9574\n","13 Validation accuracy: 0.9578\n","14 Validation accuracy: 0.958\n","15 Validation accuracy: 0.957\n","16 Validation accuracy: 0.9564\n","17 Validation accuracy: 0.9574\n","18 Validation accuracy: 0.9594\n","19 Validation accuracy: 0.9578\n"],"name":"stdout"}]},{"metadata":{"id":"e3S7vt8KWaUy","colab_type":"text"},"cell_type":"markdown","source":["#Faster Optimizers\n","The conclusion of this section is that you should almost always use Adam optimization."]},{"metadata":{"id":"tLcnCe22W8ta","colab_type":"text"},"cell_type":"markdown","source":["##Momentum optimization\n","1. $m \\leftarrow \\beta m + \\eta \\nabla_{\\theta} J(\\theta)$\n","2. $\\theta \\leftarrow \\theta - m$\n","\n","where $\\beta$ is the *momentum*, be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n","\n","In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using Momentum optimization helps a lot. It can also help roll past local optima.\n","\n","Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times\n","before stabilizing at the minimum. This is one of the reasons why it is good to have a bit of friction in the system: it gets rid of these\n","oscillations and thus speeds up convergence.\n","\n","The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than Gradient Descent."]},{"metadata":{"id":"rv9I95EWY4pT","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ldR9yiazY_yY","colab_type":"text"},"cell_type":"markdown","source":["##Nesterov Accelerated Gradient\n","measures the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum.\n","\n","1. $m \\leftarrow \\beta m + \\eta \\nabla_{\\theta} J(\\theta + \\beta m)$\n","2. $\\theta \\leftarrow \\theta - m$"]},{"metadata":{"id":"XaRSNvsKZmZC","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_HFpgZ2GZofu","colab_type":"text"},"cell_type":"markdown","source":["##AdaGrad\n","scales down the gradient vector along the steepest dimensions\n","\n","1. $s \\leftarrow s + \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta}J(\\theta)$\n","2. $\\theta \\leftarrow \\theta - \\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{s+\\epsilon}$\n","\n","where,\n","\n","step 1 accumulates the square of the gradients into the vector $s$, ($\\otimes$ represents the element-wise multiplication, which is equivalent to $s_i \\leftarrow s_i+(\\frac{\\partial}{\\partial\\theta_i}J(\\theta))^2 $ for each element $s_i$ of the vector $s$)\n","\n","step2 scales the gradient vector down by a factor of $\\sqrt{s+\\epsilon}$, ($\\oslash$ represents the element-wise divsion). $\\epsilon$ is a smoothing term to avoid division by zero.\n","\n","this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes, which is called *adaptive learning reate*\n","\n","AdaGrad often performs well for simple quadratic problems, but unfortunately it\n","often stops too early when training neural networks. The learning rate gets scaled\n","down so much that the algorithm ends up stopping entirely before reaching the\n","global optimum."]},{"metadata":{"id":"iTWGVdnscXYw","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hC_bFh0ZcZYv","colab_type":"text"},"cell_type":"markdown","source":["##RMSProp\n","1. $s\\leftarrow\\beta s + (1-\\beta)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$\n","2. $\\theta \\leftarrow \\theta - \\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{s+\\epsilon}$\n","\n","Although *AdaGrad* slows down a bit too fast and ends up never converging to the global optimum, the *RMSProp* algorithm fixes this by accumulating only the gradi‐ ents from the most recent iterations"]},{"metadata":{"id":"h_TZkG88c6AB","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n","                                      momentum=0.9, decay=0.9, epsilon=1e-10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JQxuEq_Zc-Uu","colab_type":"text"},"cell_type":"markdown","source":["##Adam Optimization\n","which stands for adaptive moment estimation,\n","\n","1. $m\\leftarrow\\beta_1 m+(1-\\beta_1)\\nabla_{\\theta}J(\\theta)$\n","2. $s\\leftarrow\\beta_2 s +(1-\\beta_2)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$\n","3. $m\\leftarrow\\frac{m}{1-\\beta_1^T}$\n","4. $s\\leftarrow\\frac{s}{1-\\beta_2^T}$\n","5. $\\theta\\leftarrow\\theta-\\eta m\\oslash\\sqrt{s+\\epsilon}$\n","\n","where $T$ represents the iteration number (starting at 1)\n","\n","Adam’s close similarity to both Momentum optimization and RMSProp.\n","\n","The momentum decay hyperparameter $\\beta_1$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999."]},{"metadata":{"id":"3O8N8tsdekH5","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ME-5V7E1eri8","colab_type":"text"},"cell_type":"markdown","source":["use the default value η = 0.001, making Adam even easier to use than Gradient Descent.\n","\n","All the optimization techniques discussed so far only rely on the *first-order partial derivatives (Jacobians)*."]},{"metadata":{"id":"Y52LEykEfDBM","colab_type":"text"},"cell_type":"markdown","source":["##Learning Rate Scheduling"]},{"metadata":{"id":"sgxA29b7fl3K","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bc75UHjjfrLB","colab_type":"text"},"cell_type":"markdown","source":["Implementing a learning schedule with TensorFlow"]},{"metadata":{"id":"9KbqQc8Rfm_G","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):       # not shown in the book\n","    initial_learning_rate = 0.1\n","    decay_steps = 10000\n","    decay_rate = 1/10\n","    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n","    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n","                                               decay_steps, decay_rate)\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n","    training_op = optimizer.minimize(loss, global_step=global_step)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MsQvdKOzf1az","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jWPYTOtaf3I6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"6c3abcf0-d3fc-4ae6-c496-7a0157779a47","executionInfo":{"status":"ok","timestamp":1540861450145,"user_tz":240,"elapsed":15416,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 5\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":118,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.954\n","1 Validation accuracy: 0.9724\n","2 Validation accuracy: 0.973\n","3 Validation accuracy: 0.9788\n","4 Validation accuracy: 0.9816\n"],"name":"stdout"}]},{"metadata":{"id":"iavnxhjYf7J_","colab_type":"text"},"cell_type":"markdown","source":["#Avoiding Overfitting Through Regularization\n","##Early Stopping\n","just interrupts training when its performance on the validation set starts\n","dropping.\n","\n","implementing this with TensorFlow is to evaluate the model on a validation set at regular intervals (e.g., every 50 steps), and save a “winner” snapshot if it outperforms previous “winner” snapshots.\n","\n","##$\\ell_1$ and $\\ell_2$ Regularizations\n","One way to do this using TensorFlow is to simply add the appropriate regularization terms to your cost function."]},{"metadata":{"id":"7eViruJGg45a","colab_type":"text"},"cell_type":"markdown","source":["Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"]},{"metadata":{"id":"IdO-lPHHg55D","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope('dnn'):\n","  hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n","  logits = tf.layers.dense(hidden1, n_outputs, name='outputs')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"52ZFDI8rhMnh","colab_type":"text"},"cell_type":"markdown","source":["Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"]},{"metadata":{"id":"cLuSmvzJhNj2","colab_type":"code","colab":{}},"cell_type":"code","source":["W1 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n","W2 = tf.get_default_graph().get_tensor_by_name('outputs/kernel:0')\n","\n","scale = 0.001 # L1 regularization hyperparameter\n","\n","with tf.name_scope('loss'):\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","  base_loss = tf.reduce_mean(xentropy, name='avg_xentropy')\n","  reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n","  loss = tf.add(base_loss, scale*reg_losses, name='loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6OvnamNlh-tR","colab_type":"text"},"cell_type":"markdown","source":["The rest is just as usual:"]},{"metadata":{"id":"7oq8Fuc9h_U8","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nWKSROqYiBdF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"1894b7e8-8578-41b2-e72c-22f491d6ce91","executionInfo":{"status":"ok","timestamp":1540862038949,"user_tz":240,"elapsed":38003,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":122,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.831\n","1 Validation accuracy: 0.871\n","2 Validation accuracy: 0.8838\n","3 Validation accuracy: 0.8934\n","4 Validation accuracy: 0.8966\n","5 Validation accuracy: 0.8988\n","6 Validation accuracy: 0.9016\n","7 Validation accuracy: 0.9044\n","8 Validation accuracy: 0.9058\n","9 Validation accuracy: 0.906\n","10 Validation accuracy: 0.9068\n","11 Validation accuracy: 0.9054\n","12 Validation accuracy: 0.907\n","13 Validation accuracy: 0.9084\n","14 Validation accuracy: 0.9088\n","15 Validation accuracy: 0.9064\n","16 Validation accuracy: 0.9066\n","17 Validation accuracy: 0.9066\n","18 Validation accuracy: 0.9066\n","19 Validation accuracy: 0.9052\n"],"name":"stdout"}]},{"metadata":{"id":"8_CD6PLriFTf","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, we can pass a regularization function to the **tf.layers.dense() function**, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"]},{"metadata":{"id":"UQ1n6h0piLEJ","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aBCQyevAiOYX","colab_type":"text"},"cell_type":"markdown","source":["Next, we will use Python's **partial()** function to avoid repeating the same arguments over and over again. Note that we set the kernel_regularizer argument:"]},{"metadata":{"id":"u-Cn5dBdiRSX","colab_type":"code","colab":{}},"cell_type":"code","source":["scale = 0.001"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Az_VEMiAiSyT","colab_type":"code","colab":{}},"cell_type":"code","source":["my_dense_layer = partial(tf.layers.dense,\n","                        activation=tf.nn.relu,\n","                        kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n","\n","with tf.name_scope('dnn'):\n","  hidden1 = my_dense_layer(X, n_hidden1, name='hidden1')\n","  hidden2 = my_dense_layer(hidden1, n_hidden2, name='hidden2')\n","  logits = my_dense_layer(hidden2, n_outputs, activation=None, name='outputs')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WbHGt8f-izx9","colab_type":"text"},"cell_type":"markdown","source":["Next we must add the regularization losses to the base loss:"]},{"metadata":{"id":"umvnlIziizDc","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope('loss'):\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","  base_loss = tf.reduce_mean(xentropy, name='avg_xentropy')\n","  reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","  loss = tf.add_n([base_loss] + reg_losses, name='loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HzdIai1ejSjA","colab_type":"text"},"cell_type":"markdown","source":["the rest is the same as usual:"]},{"metadata":{"id":"pwVdyp3GjUAt","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Aqw2NqKjWXp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"bae2584a-e24d-43cd-d576-a50f80e04545","executionInfo":{"status":"ok","timestamp":1540862391633,"user_tz":240,"elapsed":44228,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":128,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.8274\n","1 Validation accuracy: 0.8766\n","2 Validation accuracy: 0.8952\n","3 Validation accuracy: 0.9016\n","4 Validation accuracy: 0.9082\n","5 Validation accuracy: 0.9096\n","6 Validation accuracy: 0.9126\n","7 Validation accuracy: 0.9154\n","8 Validation accuracy: 0.9178\n","9 Validation accuracy: 0.919\n","10 Validation accuracy: 0.92\n","11 Validation accuracy: 0.9224\n","12 Validation accuracy: 0.9212\n","13 Validation accuracy: 0.9228\n","14 Validation accuracy: 0.9224\n","15 Validation accuracy: 0.9216\n","16 Validation accuracy: 0.9218\n","17 Validation accuracy: 0.9228\n","18 Validation accuracy: 0.9216\n","19 Validation accuracy: 0.9214\n"],"name":"stdout"}]},{"metadata":{"id":"RXxUrarqjZpf","colab_type":"text"},"cell_type":"markdown","source":["##Dropout\n","at every training step, every neuron (including the input neurons but excluding the output neurons) has a probability $p$ of being tem‐\n","porarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step.\n","\n","The hyperparameter $p$ is called the dropout rate, and it is typically set to 50%. After training, neurons don’t get dropped anymore."]},{"metadata":{"id":"1k0GGaIvkfp-","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_yc5eYWbkhu0","colab_type":"code","colab":{}},"cell_type":"code","source":["# A placeholder op that passes through input when its output is not fed.\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","dropout_rate = 0.5 # == 1 - keep_probability\n","X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n","\n","with tf.name_scope('dnn'):\n","  hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name='hidden1')\n","  hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n","  hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name='hidden2')\n","  hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n","  logits = tf.layers.dense(hidden2_drop, n_outputs, name='outputs')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9_QjoS9CmIW5","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-e64l9R6mKhL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"9d9ee8d0-819f-4dc1-b3f1-fce6016fa97a","executionInfo":{"status":"ok","timestamp":1540863160429,"user_tz":240,"elapsed":75959,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":132,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.9254\n","1 Validation accuracy: 0.9452\n","2 Validation accuracy: 0.9492\n","3 Validation accuracy: 0.9566\n","4 Validation accuracy: 0.9618\n","5 Validation accuracy: 0.9602\n","6 Validation accuracy: 0.96\n","7 Validation accuracy: 0.9674\n","8 Validation accuracy: 0.9688\n","9 Validation accuracy: 0.9702\n","10 Validation accuracy: 0.9678\n","11 Validation accuracy: 0.9684\n","12 Validation accuracy: 0.9714\n","13 Validation accuracy: 0.9708\n","14 Validation accuracy: 0.971\n","15 Validation accuracy: 0.971\n","16 Validation accuracy: 0.973\n","17 Validation accuracy: 0.9716\n","18 Validation accuracy: 0.9732\n","19 Validation accuracy: 0.974\n"],"name":"stdout"}]},{"metadata":{"id":"GHwOSGhBmQqU","colab_type":"text"},"cell_type":"markdown","source":["Just like you did earlier for Batch Normalization, you need to set **training** to **True** when training, and to **False** when testing.\n","\n","If you observe that the model is overfitting, you can increase the dropout rate (i.e., reduce the keep_prob hyperparameter). Conversely, you should try decreasing the dropout rate (i.e., increasing keep_prob) if the model underfits the training set."]},{"metadata":{"id":"A5VxR8OCmkHJ","colab_type":"text"},"cell_type":"markdown","source":["##Max-Norm Regularization\n","for each neuron, it constrains the weights $w$ of the incoming connections such that $|| w||_2 ≤ r$, where $r$ is the max-norm hyperparameter and $||\\cdot||_2$ is the $\\ell_2$ norm.\n","\n","We typically implement this constraint by computing  $|| w||_2 $ after each training step and clipping $w$ if needed $(w\\leftarrow w\\frac{r}{||w||_2}$)\n","\n","Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"]},{"metadata":{"id":"MA8ZUUA3nvQI","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","learning_rate = 0.01\n","momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I1StQ9nVn3Us","colab_type":"text"},"cell_type":"markdown","source":["Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the **clip_by_norm()** function. Then we create an assignment operation to assign the clipped weights to the weights variable:"]},{"metadata":{"id":"2A8aIMccn6GV","colab_type":"code","colab":{}},"cell_type":"code","source":["threshold = 1.0\n","weights = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n","clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n","clip_weights = tf.assign(weights, clipped_weights)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"11DD3EImoOH7","colab_type":"text"},"cell_type":"markdown","source":["We can do this as well for the second hidden layer:"]},{"metadata":{"id":"Wi80xIUQoOoo","colab_type":"code","colab":{}},"cell_type":"code","source":["weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n","clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n","clip_weights2 = tf.assign(weights2, clipped_weights2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XCAJUhOPoQ23","colab_type":"text"},"cell_type":"markdown","source":["Let's add an initializer and a saver:"]},{"metadata":{"id":"jFMWZEvNoSiR","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rakafWh6olAY","colab_type":"text"},"cell_type":"markdown","source":["And now we can train the model. It's pretty much as usual, except that right after running the **training_op**, we run the **clip_weights** and **clip_weights2** operations:"]},{"metadata":{"id":"5uveVIkeopPR","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"46PmDa8ZorUf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"366521cd-b409-41d3-aa16-3efdd201f335","executionInfo":{"status":"ok","timestamp":1540864069129,"user_tz":240,"elapsed":141516,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  init.run()\n","  for epoch in range(n_epochs):\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","      sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n","      clip_weights.eval() # apply here\n","      clip_weights2.eval() # apply here\n","    acc_valid = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n","    print(epoch, 'Validation accuracy:', acc_valid)\n","    \n","  save_path = saver.save(sess, './my_model_final.ckpt')"],"execution_count":148,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.9568\n","1 Validation accuracy: 0.9696\n","2 Validation accuracy: 0.9716\n","3 Validation accuracy: 0.9772\n","4 Validation accuracy: 0.9772\n","5 Validation accuracy: 0.9774\n","6 Validation accuracy: 0.9822\n","7 Validation accuracy: 0.981\n","8 Validation accuracy: 0.98\n","9 Validation accuracy: 0.9824\n","10 Validation accuracy: 0.9822\n","11 Validation accuracy: 0.9852\n","12 Validation accuracy: 0.9824\n","13 Validation accuracy: 0.984\n","14 Validation accuracy: 0.9842\n","15 Validation accuracy: 0.9842\n","16 Validation accuracy: 0.984\n","17 Validation accuracy: 0.9834\n","18 Validation accuracy: 0.9842\n","19 Validation accuracy: 0.9844\n"],"name":"stdout"}]},{"metadata":{"id":"ZidLtpUqpbPH","colab_type":"text"},"cell_type":"markdown","source":["The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a **max_norm_regularizer()** function:"]},{"metadata":{"id":"ml-DkOEGpc5_","colab_type":"code","colab":{}},"cell_type":"code","source":["def max_norm_regularizer(threshold, axes=1, name='max_norm', collection='max_norm'):\n","  def max_norm(weights):\n","    clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n","    clip_weights = tf.assign(weights, clipped, name=name)\n","    tf.add_to_collection(collection, clip_weights)\n","    return None # there is no regularization loss term\n","  return max_norm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1lcxebaap-XK","colab_type":"text"},"cell_type":"markdown","source":["Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the **kernel_regularizer** argument:"]},{"metadata":{"id":"zYIXBRkAqEgu","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","learning_rate = 0.01\n","momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CYM1xCfeqGTX","colab_type":"code","colab":{}},"cell_type":"code","source":["max_norm_reg = max_norm_regularizer(threshold=1.0)\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5n4eK2oIqOHQ","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"273UhdP3qRM6","colab_type":"text"},"cell_type":"markdown","source":["Training is as usual, except you must run the weights clipping operations after each training operation:"]},{"metadata":{"id":"MsrJf5lTqTfH","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hP3VaVgIqUzM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"7a7958d9-640f-43ca-cab8-c81e793e8db0","executionInfo":{"status":"ok","timestamp":1540864404405,"user_tz":240,"elapsed":133591,"user":{"displayName":"Bin Liu","photoUrl":"https://lh3.googleusercontent.com/-U3EW8bZQfB4/AAAAAAAAAAI/AAAAAAAAAAA/cEBAusQDE7w/s64/photo.jpg","userId":"15040736409405760339"}}},"cell_type":"code","source":["clip_all_weights = tf.get_collection('max_norm')\n","\n","with tf.Session() as sess:\n","  init.run()\n","  for epoch in range(n_epochs):\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","      sess.run(clip_all_weights)\n","    acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","    print(epoch, \"Validation accuracy:\", acc_valid) \n","\n","  save_path = saver.save(sess, \"./my_model_final.ckpt\") "],"execution_count":155,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.9558\n","1 Validation accuracy: 0.9684\n","2 Validation accuracy: 0.9742\n","3 Validation accuracy: 0.9748\n","4 Validation accuracy: 0.977\n","5 Validation accuracy: 0.9802\n","6 Validation accuracy: 0.9782\n","7 Validation accuracy: 0.9798\n","8 Validation accuracy: 0.9818\n","9 Validation accuracy: 0.9816\n","10 Validation accuracy: 0.983\n","11 Validation accuracy: 0.982\n","12 Validation accuracy: 0.9832\n","13 Validation accuracy: 0.983\n","14 Validation accuracy: 0.9838\n","15 Validation accuracy: 0.9838\n","16 Validation accuracy: 0.9838\n","17 Validation accuracy: 0.9836\n","18 Validation accuracy: 0.9838\n","19 Validation accuracy: 0.9834\n"],"name":"stdout"}]}]}